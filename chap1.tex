% -*- outline-regexp: "%--* ";  -*-
%- Preliminaries
\message{Chapter 3}
%% These are defined at the end of the mathphdthesis.sty
\titlepg
\signaturepage
\altcopyrightpage
\abswithesis
\ackpage
\pagenumbering{arabic}

\tableofcontents  

% Note that the text in the [] brackets is the one that will
% appear in the table of contents, whilst the text in the {}
% brackets will appear in the main thesis.
%\setcounter{page}{2}

\chapter[INTRODUCTION]{Introduction}\label{intro}

\typeout{Chapter 1: Introduction}
\section{Overview and introduction}

This thesis puts forward the argument that better models may be built
if we allow the representation of component parts of a model to change
according to their state. The nature of their interaction with other
components, and the needs and states both of the other components and
of the model as a whole. Practitioners in many fields often appeal to
Occam's Razor when selecting models, explanations or solutions to
problems and, in some way, the proposed strategy embeds an analogous
winnowing in the very structure of a model -- a representation of the
system that best meets the requirements is sought, and changes are
made in the representation as the conditions within the model change.

Such a strategy has a number of potential benefits:
\begin{itemize}
\item[--] we can make many simple representations, submodels, for a niche
        (in the sense used in \cite{gray2006nws} and \cite{gray2014}) in the
        model, each of which deals well with a particular part of the
        submodel's domain;
\item[--] the comparative simplicity of these representations effectively 
      reduces the number of potential code paths within a model at any
      given moment, since representations do not have to cope with
      edge cases, they merely indicate that they are entering a marginal
      or inappropriate domain;
\item[--] we can use analytic representations which are more efficient at
      representing large numbers of entities;
\item[--] we can use individual-based representations which capture
      the fine-scale dynamics that dominate when we are dealing with
      discrete events or low numbers of entities;
\item[--] we can choose representations that make the best use of available
      data within the model, or can ask for better representations
      in the ensemble;
\item[--] it is simple to incorporate code to track information about
      representation changes, relative execution speed, and cumulative
      error into the modelling system;
\item[--] we can include (or not) agents that identify the emergence of 
      perverse dynamics within the system;

\listintertext*{and} 
\item[--] we can decouple the production of the results from processes
      which simulate the systems and subsystems being modelled.
\end{itemize}

This strategy for building models makes it simple to address the
questions ``\emph{How do we deal with situations 
where the assumptions that underpin our representation no longer hold?}'' 
and ``\emph{How do we manage the execution of submodels which simulate
systems or entities with multi-modal behaviour?}'' quite straightforward.

Consider this example: rather than a single representation for the
population of a coastal city, we may have a number of
different \emph{submodels} with different levels of aggregation and
different temporal or spatial scales.  In a simulation of a cyclone
season, we may start with a simple single age-histogram representation.
As a tropical storm builds we may disaggregate the histogram,
appropriately distributing the population to finer age-histograms
associated with localities throughout the region.  As it approaches
the coastline, the essentially static representations which lie in
areas likely to suffer damage are converted into \emph{agents}  -- 
instances of running submodels -- which represent households.
Shortly before the cyclone reaches the point where damage occurs, we
may resolve the representation further, instantiating emergency
response agents and converting households agents to individuals at
risk.  In the aftermath, aggregation may occur in regions where there
are no acute effects, but other parts of the system may remain finely
resolved.

In this example, we change the representations to deal with both of
the questions above.  We initially assumed that for most of the
purposes of the simulation, our population could be treated as
relatively homogeneous, and may have kept aggregate information about
population distribution, wealth and demographic characteristics, but
as the storm hits, the simulation needs to change the state of a
portion of the population (those with damaged property, for example),
and we have to refine our representation.  Similarly, the behaviours
following the storm are modally different: those who live in protected
areas are mostly free to carry on with essentially the same ``normal''
representation, but those who are living in damaged areas must engage
in quite different activities, and may have quite a different exposure
to risks.

Adaptive approaches commonly occur in numerical techniques for
numerical approximation (regression, root finding, and parameter
estimation for example), numerical solutions for systems of
differential equations, feature detection and recognition, control
systems and route planning.  Often these approaches involve adjusting
the size of the domain considered (subdivisions or step size), or the
rates associated with a process. In the case of route planning, sets
of routes may be marked as ``impassable'' as data becomes available,
triggering a reassessment of the set of possible routes. More
broadly, \emph{domain adaptation} describes a general approach where a
model or system adjusts itself to the data it works with -- one of the
canonical examples is a Bayesian spam filter which includes a user's
assessment of whether email is spam or not in its subsequent
assessments. A common trait these adaptive techniques share is that
the \emph{algorithm} which processes the data remains essentially the
same.

\Cite{DBLP:ZhangZR16} describe a system for the detection and tracking
of pedestrians which selects the algorithms and parameters to be used
in the analysis of segments of data based on the nature of the data it
is given. This approach has qualitative similarities to the approach
suggested in this work, since the whole method of evaluation changes
based on its input, rather than adjusting the scales or domains.

Discrete changes in the behaviour of a system, or part of a system,
are commonplace. The scales of systems that exhibit switching
behaviour range from a molecular level, such as the behaviour of
freezing liquids, through to climatic changes.  The discussion of
``tipping points'' has increased dramatically in the last decade
\citep{bhatanacharoentipping}, indicating a broad recognition that
systems' dynamics can (and do) switch rapidly from one mode to another

\Cite{huston1988new} is an early review paper which deals with
individual-based modelling as an alternative to purely equation-based
models which deal with population level data.  It opens with the
observation to the effect that mathematical models in ecology often
make the assumptions that ``[individual organisms] can be described by
a single variable, such as population size'' and that ``each
individual is assumed to have an equal effect on every other
individual'' [because locations are ignored]. They argue that
individual-based models are able to incorporate dynamics across scales
and that the fine scale dynamics experienced by individuals plays a
significant role in many of the population scale patterns observed in
ecological studies.

Individual-based or super-individual-based models are not a panacea.
Models of this sort may become quite costly as the number of
individuals or the interactions between individuals or
super-individuals grows, and small discrepancies between the
``behaviour'' or parameterisation of the modelled entities and their
real counterparts may produce large discrepancies at the population
level. Many of the parameters that may influence the life-history of
organisms at an individual level are difficult or impossible to
estimate \emph{in situ}, and so the effects of individuals' modelled
behaviours or processes may not scale well when incorporated in larger
systems.

It is well known that exogenous factors, such as changing
environ\-mental conditions (\eg the availability of water or prey
can significantly alter the behaviour of organisms).  Migrations are
common in many populations, reasons include moving to particular
locations for breeding, avoiding seasonal scarcity of resources, or
the encroachment of competing species such as \textit{Homo sapiens}.
These examples are relatively predictable, and typically involve a
homogeneous response from the migrating population.
\Cite{ward1985behavioural} discusses the behaviour of
lynxes in response to declining prey populations. In this study, two
distinct behavioural responses prevailed: some animals choose a
nomadic lifestyle as a means of optimising their likelihood of hunting
success, while others remain in their own territory.  Though Ward's
sample size was small, the distinct responses suggest that lynx
populations respond to prey scarcity in a heterogeneous way and, as a
result, may be less amenable to modelling as a population.

Toxicants and other chemical contamination in a terrestrial
environment are likely to have a proscribed footprint; where toxicants
are air-born or water-born they may disperse more readily and
advection may increase the area of potential contact.  In models of
these situations, the populations can be fragmented into a number of
distinct, possibly mobile, sub-populations. \Cite{zala2004abnormal}
presents a useful review of the the effects of behaviour disrupting
contaminants in a broad range of animals.  Even in simple situations,
where the behaviour and viability of members of a population are not
compromised, the consequences of contact may percolate through the
food-chain to higher order predators. \Cite{swan2006toxicity} has
found that very low levels of the anti-inflammatory drug diclofenac
are fatal to old world (\textit{Gyps}) vultures which acquire it by
scavenging carcasses of dead cattle, and the bio magnification and
effects of DDT in the food chain have been well discussed in the
scientific literature since 1964.

When an altered behaviour is associated with the spread or
reproduction of organisms; in these cases, the scope for positive
feedback is increased, and the dynamics can diverge rapidly from
representations that are adequate for an unperturbed system.  The
effects of \textit{Toxoplasma gondii} on rats
\cite{berdoy2000fatal} is an ideal example: rats exposed to
infected cat feces lose their innate fear of cats -- the positive
reinforcement on the spread of \textit{T. gondii} afforded by this
leads to greater potential for the pathogen to infect more cats, and
hence, more rats. \Citet{dobson1988population} investigates the
population dynamics of these kinds of interactions, and present a
useful approach to incorporating these effects into analytic
models. This paper explored the population dynamics of parasite-host
systems where the parasites influenced host behaviour found that the
reproductive capacity of the host populations could be significantly
modified by the behaviour altering parasites. \Citep{dobben1952food}
observes that roaches infected with \textit{Lingua intestinalis} were
three to five times more numerous in cormorant catches than in the
roach population of the IJsselmeer\footnote{A large lake in the
Netherlands.} (as estimated from commercial catches), suggesting that
something in the fish's behaviour makes them more susceptible to
capture. \Cite{poulin1994meta} assesses the effect on host behaviour
in a number of host-parasite pairings, and found that the parasites
had a significant effect on the behaviour of their hosts.  In the
cases addressed in these papers, the process in question is predation,
and the infected individuals are often either disproportionately
preyed upon, or involved in the parasite's reproductive cycle.

The situation is more complex when there are endogenous reasons for
fundamental changes in an organism's basic dynamics. This kind of situation
can induce radical, even pathological, changes in behaviour.  Social
animal populations may behave in quite strange ways when their
population density grows too large or the population's social profile
is disrupted.  A seminal (and \emph{grim}) example of this is described
in~\cite{calhoun1973death}. Calhoun recounts an experiment in which
mice are confined in a domain where all their physical needs were met,
all possible sources of mortality apart from senescence and death by
injury were excluded, and there was no possibility of emigration.
Social and behavioural disintegration began to manifest in the third
generation (day 315), and the population went into terminal decline
after 560 days, and for all practical purposes the social organisation
had collapsed utterly.

Calhoun discounted the population density as the cause of the social
disintegration, rather attributing the collapse to the inability of
young adults to engage in ``normal roles'' due to high competition for
the \emph{social niches} which were filled by older, more established
mice. The behavioural changes attending the social collapse did not revert
to more normal when population levels dropped (past 560 days).

Systems which incorporate a number of components which have the
properties we have described may exhibit dynamics which are difficult
to address in a conventional way. Software development principles
encourage loose coupling and narrow interfaces between submodels, but
as the complexity and range of functional elements being modelled
increases, the number of potential interactions for a component grows
dramatically.  With this growth, ensuring the robustness of a submodel
over its range of potential states and interactions may become
prohibitively difficult, simply because it is difficult to check all
possible interactions and execution paths.  Even with very clean,
robust code, the probability of poor interactions during a run
increases dramatically as the number of lines of code in submodels
increases. In my own experience in modelling human-ecosystem
interactions, the demand for richer models of ecosystems and more
detailed models of human activity has driven an increase in the number
of types of submodels and their complexity by roughly a factor of four
every seven years. These rates may be limited by the
capacity of the hardware available, as well as the human limits which
arise in the design, implementation and testing of models.

\Cite{fultonPhD} surveys the effects of the
complexity and structure in models of marine ecosystems to couch the
discussion, but the issues are more general.  Fulton articulates five
essential conclusions:
\begin{itemize}
\item[---] indicator groups can be used to monitor the effects of
management strategies and environmental condition
\item[---] designing management strategies based on a small part of
the trophic system can have perverse consequences
\item[---] multi-species models have more discriminatory power with
respect to environmental conditions and the effects of external
forcing

\listintertext{and}
\item[---] the use of different models (model-configurations) as a
means of broadening the number of perspectives on the modelled system
is of significant value in many ways.
\end{itemize}

While the fifth -- that shallow, enclosed marine systems may
be predisposed to eutrophication -- seems specific to the domain on
the surface, it actually alludes to what may be a more pervasive
condition, namely that isolated, constrained environments may be more
vulnerable to radical switching.  The remaining conclusions can be used
to guide the development of the control mechanisms for an adaptive
hybrid-model.  The mechanisms described in \Cfive have been designed with
these points in mind: we need a reasonable number of grouping to be
assessed, and that analysing them in different ways can prove
illuminating. 

\Cite{fulton2010approaches} provides a comprehensive overview
of \emph{end-to-end} or \emph{whole-of-system} models drawn from the
literature associated with marine ecosystems and species. 
The paper discusses the major types of models in use, qualitative or
conceptual models, bio-geochemically based models, models of a systems
trophodynamics, and coupled or hybrid models. It goes on to expand on
points made in the FAO guidelines on ecosystem model development for
fisheries; most notably that the physical scales in the representation
should be appropriate for the model (finer is not implicitly better), and
that
\begin{quote}
    At the end of this process the necessary components will need to
    be represented at the appropriate scales in prototype or final
    model(s). It is important to reemphasize here that there is no one
    single right model. All models have problems and it is best (where
    possible) to use a range of models that can address the question
    in different ways.
\end{quote} [\cite{FAO2008ecosystems}, p. 78].
While the thrust of the quotation is more in the direction of
conventional ensemble modelling, the observations apply equally well
to the argument that using the best representation at the
time \emph{within} a model is preferable to settling for a less
apposite representation.

Fulton also addresses issues associated with parametric uncertainty
and structural uncertainty.  This latter element is particularly
significant in systems where the prevailing structures may switch
between a number of distinct, locally stable modes.

\Cite{rose10:1} identifies and explores nine significant issues in
marine ecosystem models, and takes particular care in addressing the
increasing need to incorporate climatic changes into end-to-end
ecosystem models.  While their focus is on marine systems, the issues
they highlight are pertinent to the modelling of a wide range of
systems.  The salient issues they pose fall into essentially four groups:
\begin{itemize}
\item[1,2,4,5] deal with the nature of the submodels used to
represent a component of the modelled system;
\item[3] managing multiple physical scales;
\item[7 and 8] the monitoring and assessment of the model as a whole
and of the component submodels;
\item[6, 9] the access or development of an interested community and
tools to make the coupling of models or components of models feasible.
\end{itemize}
In some ways the paper acts as an annotated road map and poses the
question, ``Where should we all meet?''  A great deal of the work in
this thesis has been concerned with the first three of these
categories, and may offer some aid with the last.


\typeout{Chapter 1: Historical work}
\section{Historical work}

Many individual-based models incorporate environmental characteristics
that influence the behaviour of of the individuals
simulated. \Citet{Botkin72:2} and~\citet{deangelis1978model} are
important early examples: Botkin \etal modelled the effect of
spatially explicit environmental conditions on simulated trees (rather
than stands or coupes) in a mixed species population in
North America; in the case of Botkin \etal, the model was used to
explore the distribution of fish (modelled as individuals) in a
speculative body of water with a known distributions of temperature
and food availability. Both of these models simulated the 
dynamics resulting from the physical conditions real plants and
animals might encounter. 

The coastal marine ecosystem models based on the \InVitro framework
in~\cite{gray2006nws} used different representations for the organisms
based on their life-stage. As organisms that comprised the benthic
habitat matured their representations would change to suit their niche
in the system.  In this case the sequence of transitions was
determined before compilation of the model; juvenile biomasses could
be represented either by gridded cellular automata or by polygonal
clouds which changed shape as they were advected, while adult stages
could be represented by several different submodels which might be
optimised either for speed or for spatial fidelity.  This model was in
most other ways similar to conventional agent-based modelling of the
time.

%\cite{gross2002multimodeling} describes using this approach as a
%basic element underpinning their model, ATLSS, of the Everglades
%region in the U.S.A.

\Cite{bobashev2007hybrid} describes a model of epidemic simulation in
which the representation of populations or portions of populations are
decided based on the number of infected individuals relative to a
nominated trigger value. This model demonstrates that there is an
advantage to changing representation in terms of computational
efficiency and the fidelity of the model. The published model in
\Ctwo is similar: the rule governing the
switching from one representation to another depends on the state of
the system: switching occurs when some monitored quantity crosses a
nominated boundary. The problem of contact with infection and contact
with a contaminant is similar, though the treatments are quite
different. In~\citet{bobashev2007hybrid}, an individual moves
from \texttt{Susceptible}, through \texttt{Exposed},
to \texttt{Infectious} and then \texttt{Recovered}; in contrast, when
individuals with contaminant loads in \Ctwo
leave the region where contamination is possible, they are subsumed
back into the population and their data is incorporated so that the
individual contaminant profiles are maintained and subject to
depuration.

The \InVitro based model discussed in~\cite{gray2014}
and~\cite{fulton2009crossingscales} simulates the effect of a number
of strategies for managing human recreational and industrial activity
along the northwestern coastline of Australia. The model 
incorporated distinct individuals, super-individuals, mean field
submodels, submodels which were equivalent to cellular automata and
systems of differential equations. In this model, the representation
of whales was notable because individual whales would be transferred
to another (mostly inaccessible) domain when their migration took them
outside the model's domain: the whales would be maintained in a
rudimentary way until it was time for them to migrate back into the
model domain. Other agents within the model (predators, wildlife
management and tourism operators) were influenced in their decision
processes by the presence or absence of whales in the model domain.
%%% brief comment on trends in the field???


\section{Structure }
Models of complex systems usually incorporate alternative code paths
or expressions in a component to deal with situations where there are
fundamentally different dynamics or properties by testing for these
conditions at each potential fork in the code. This can engender a
complicated network of potential execution paths through the model.
In contrast, the approach discussed in this thesis addresses this
problem by constructing the models as an ensemble of agents which can
cede their role to another representation which is more suitable when
the need arises.  A resident population might thus be represented by a
single \emph{population\/} agent, a set of \emph{individual-based\/}
agents, \emph{super-individual\/} agents or by some mixture of these
representations.  It may be that particular representations are unable
to respond to the conditions they encounter: a population based agent
may be unable to to interact with a contaminant plume, for example,
and a representation that can is required for the interaction to
occur.  Should contact with the plume no longer become necessary, the
system ought to be able to convert the representation back to its
original form, with information about the contaminant load maintained
(and depurated, if appropriate) in the original representation.

The decision to change the representation of a submodel occupying a
niche in the model is based on the state of the system, the
capabilities (or incapability) of the agents in the system and the
objectives of the modeller -- in configuring a model, we might
prioritise speed over accuracy in a real-time simulation, a computer
game or a combat training simulator; conversely, for a scientific
extrapolation of the state of a harbour for each of a number of
development scenarios, we might choose accuracy over speed.

Representing the state of the model, either as a whole or of its
constituent parts, is not simple: not only may the sub-model mix in
the niches of the model vary through time, but their dependences
on other components and niches may change as the state of the model
changes. A model which contains a ``whale spotting'' tourism venture
may follow the activities of whales at particular times of the year,
but be utterly indifferent to the whales at times when there is little
likelihood of their presence in an accessible location. Similarly, the
association of entities represented by a population-based model may
need to be maintained if the population dis\-aggregates into agents
based on super-individuals (small cohorts) or agents representing
individuals.

The interplay of factors like these make a simple vector-based
encoding mechanism for the states of a model and its components
awkward, thus we turn to a metric space whose elements are trees with
a finite number of weighted, labelled nodes. This simplifies the
comparison of possible ways to fill the niches in the model for a
given global state, and makes available any algorithms (particularly
useful are clustering) which depend only on the properties of a metric
space.

The decision to change representations can be made by an agent that
recognises that it is unable to continue in the conditions in which it
finds itself (akin to the code-path decisions in more traditional
models), or by a similar assertion from some higher agency
(a \emph{monitor\/} in the discussion which follows) which assesses
states more broadly.  In the case of an agent determining that it
needs to change, such as a penguin moving from the ``nestling''
submodel to the ``juvenile'' submodel, this can be effected directly;
the more general (and in this case, burdensome) strategy would
for a \emph{monitor\/} to flag the desired state change and then act
on it when appropriate.

The models explored in this work bear a resemblance to a multitasking
operating system, but, unlike an operating system, the model's ``kernel''
must maintain temporal ordering in the execution of agents \ldots the
start times within the agent queue must be strictly non-decreasing.
Interactions between agents are largely mediated by the kernel and
new agents may be created or removed with relative ease.  Choosing
this as an organisational template means that there are many patterns
to serve as templates for further development.

All of the submodels in the example model presented in \Cfive are able
to act to some degree as a kernel themselves -- in a sense, models can
be nested.

\section{Scales}
The natural time step or spatial scales of a model may change if one
or more of its constituent submodels changes its representation.
This seems like an obvious statement, but many models are structured
with quite carefully chosen time steps and spatial scales, and they
may behave quite poorly when these scales are changed.  Models of
individual organisms are likely to require much smaller temporal and
spatial scales than representations at a population level, so a model
which seeks to accommodate both possibilities must necessarily be able
to adapt to the scales that are important at the moment.  In practice,
changing spatial resolution is relatively straightforward if submodels
do not rely on knowledge of the underlying implementations of other
submodels.  The sorts of causal issues that accompany predation, for
example, tend not to arise when we move from a finely gridded
landscape to a coarser version.

Changes in temporal scale can be more problematic, however. Time
influences causality in a way that space does not. Deciding how to
manage the flow of time in an ecological simulation model is one of
the first decisions in its design.  Many ecological models have been
constructed as a large set of arrays containing state variables which
are inspected and updated in the body of an event loop (or many
loops). Some models achieve temporal optimisation by dividing the
arrays into various groups of fast-stepping and slower-stepping
variables, only dealing with the necessary parts of the system at each
time step (\emph{variable speed splitting\/} as
in~\cite{walters2000ecosystem}, for example). \Cite{gray2006nws}
and~\cite{gray2014} allow agents to dynamically determine their own
time step based on their state --  time steps may be truncated, or
changed for their next turn in response to their situation. There is a
trade-off in this: with a variable speed splitting approach, we can
calculate all values based on a temporally coherent set of data, and
update them all in one pass; in contrast, the dynamic time stepping
approach means that each interaction is essentially conducted in
isolation, and the consequences of a set of interactions may be
dependent on the order in which the interactions occurs. Both variable
speed splitting and dynamic time step selection are flexible enough to
support representational changes for entities, but the greatest
advantage comes from constructing the submodels to be robust with
respect to arbitrary time steps over a reasonable domain.  If a model
is consistently run with time steps which are too long or too short,
the model or system needs to be able to initiate a change to a more
appropriate representation.

Inappropriate or incommensurate time steps can pose a real problem:
while the interactions between submodels with short time steps and
submodels with long time steps may be managed, at least to some degree,
by accumulating changes to the slower model and applying them during
the slower model's time step,  this is not an ideal solution. One of
the major risks this approach poses is a of distortion of resource
availability which is dependent on the order in which agents are
executed. This sort of error can artificially inflate or deplete
apparent resources in a seemingly random fashion, and render the
results of the simulation useless.

%% It also highlights the need for care to be taken in the construction of
%% submodels and the choice of submodels to be considered when
%% constructing a model.

The principles which have guided the coupling of models remain
salient, particularly those aspects associated with issues of
coherence in time and space. While matching time steps isn't essential,
the discrepancy between the time steps of interacting models should be
limited by the magnitude of the changes which may occur as a result of
interactions -- large changes may call for small time steps.

\section{Outline}
The paper which forms the body of \Ctwo
develops a model of organisms that periodically move through a region
subject to plumes of contaminant.  The model is capable of modelling
the organisms either with a population-based representation or with an
individual-based representation.  This model is run in three
configurations: purely population based, purely individual-based and
as a hybrid where the individual-based representation is used when it
is possible for any of the population represented to come into contact
with the contaminant, and with the population-based elsewhere. An
essential notion that was treated lightly in this paper is developed
much more fully, namely that for a model to allow an oscillation
between representations, additional data must be passed between them,
maintained and possibly adjusted in order to preserve consistency
across transitions.

The purpose of the model and its runs is to compare both the execution
speed of the simulations and the fidelity of the simulation with
respect to the contaminant loads of the simulated population.

\Cthree was published in a special issue
of \emph{Frontiers in Environmental Science\/}. It considers the 
properties needed for a more complex evaluation of possible
configurations of a running model, and develops a speculative model as
a platform for discussion.  To support the dynamic assessment and
selection of model configurations, the paper introduces a metric space
based on a tree structure. The metric space allows us to calculate
distances between configurations and to reduce our potential search
spaces by identifying clusters of representations that are largely
similar in their constitution.

\Cfour presents a modified version of the appendix included
in \Cthree. The modifications make the structures easier to manipulate,
and code to perform mathematical operations on these objects forms
the basis for the selection process in the realised model discussed
in \Cfive. 

An implementation of a framework for the model described in \Cthree is
discussed in \Cfive.  This implementation serves as something more
than a straw-man, but less than a fully developed model.  It is an
example of a system where many of its components have more than one
possible representation, and these representations can change in
response to their own state, the states of other agents, or even the
requirements of other agents.  Where possible, the options for model
selection differ in an important and fundamental way: they spans the
range from individual-based representations, through intermediates
which represent a number of individuals, to conceptually continuous
models. These changes are, in some sense, analogous to the transition
from the set of integer to the set of real numbers.  A model which
demonstrated the utility of adaptive representations without changing
the ``cardinality'' of the system would have been much simpler, but it
would have missed one of the most important parts of the problem.

\begin{center}
---
\end{center}

The corpus of code in the framework is a little under 28,500 lines of
\Scheme code and is freely available at\\
\makebox[\textwidth]{\repos}

The interpreter/compiler used in this work is \Gambit developed by
Marc Feeley.  It has a thriving community, excellent support and
integrates well with \CC and \Cpp, and is available from the GitHub
repository\\
\makebox[\textwidth]{\URL{https://github.com/gambit}}

While the model does not require \SLIB by Aubrey Jaffer, \SLIB
provide a broad range of useful functions. \SLIB is available at\\
\makebox[\textwidth]{\URL{http://people.csail.mit.edu/jaffer/SLIB.html}}



