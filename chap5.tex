% chap5.tex (Chapter 5 of the thesis)

%% classes are refered to in textit, syntactic elements are in texttt
%% filenames are in pandora
\message{Chapter 5}

\chapter[AN EXPLICIT IMPLEMENTATION]{An explicit
implementation\footnote{The model this chapter refers to is released
under the GPLv3 and available at \repos.}}
\WeAreOn{\cfive}\label{explicitmodel}


\Ctwo demonstrated that switching models can provide benefits in
fidelity and efficiency, when compared with non-switching alternative
models.  The example developed in \Cthree describes a model of a small
trophic network to use to explore how a general model might be
constructed. Here, we describe an explicit implementation of a system
based on \Cthree. The role of this implementation is to provide a
concrete starting point for further exploration, development and
critique.  In this chapter, we describe an implementation in accord
with the discussion in \Cthree for use as an exploratory tool.  This
chapter discusses the reference model (\ReModel) and the
reasons certain approaches have been taken.

\section{Design considerations}
The general design of this example framework follows conventional
patterns for an agent-based model, and it was influenced by my
previous work (\cite{lyne1994pmez5, gray2006nws, gray2014}) designing
and implementing the core of operational models for management
strategy evaluation (MSE) dealing with anthropogenic effects on
marine ecosystems.  The principle behind MSE and adaptive management
is that candidate strategies for managing the effects of human
activity are explored in simulation\footnote{This is effectively
  impossible to do in the real world, since the application of a
  strategy has a high likelihood of changing the baseline for other
  candidates in the trial.} Adaptive strategies actively monitor
conditions in their domain and change management strategies to suit
the observed conditions and dynamics of the
system. (\cite{walters1976,smith1993,polacheck1999,sainsbury2000,keith2011})
Each of these three major projects has provided the opportunity to
explore the issues associated with modelling increasingly complex
systems over increasingly large areas.

The model in \cite{lyne1994pmez5} was a purely individual-based model
with integer time steps, though the time steps varied with the species
being simulated. Like the example model, all activity was coordinated
by a queue of entities that were sorted on when they were next due to
run. This model worked in a floating point space and the simulated
organisms exhibited modal behaviour: searching, pursuit of prey,
eating and movement in a possibly drunkard's walk with migratory
weighting applied in the case of some species.

While \cite{gray2006nws} used many of the approaches from its
predecessor, its focus was considerably more broad, and required a
much more flexible approach to the problem of simulating the
interactions of human activity and components of local ecosystems. In
this model time was treated as a continuous variable, and a number of
new kinds of simulated entities were required to reflect the
activities influencing the system and the management of these
activities. This broader canvas required a more abstracted view of
what constituted an ``agent'' in the system, and extended
\cite{lyne1994pmez5} in the way it dealt with the simulated
populations, treating some as collections of instances of individual-
or super-individual-based representations and others as aggregate
representations.  The model also used distinct representations for
different life-stages of some species within the model.  Care was
taken so that entities within the model as a whole could interact
consistently regardless of their particular representation.

\cite{gray2014} was like \cite{gray2006nws} in that it addressed the
problem of the regional management of human activity as it impinged
upon the coastal ecosystems, but it incorporated a much more diverse
set of components, including terrestrial land-use, local economies,
marine tourism and recreation, and coastal industries such as salt
production and fishing. The major difference between this model and
\cite{gray2006nws} was that in this model, the simulated whale-sharks were
removed from the common domain as a part of their annual migration and
reproduction cycle. 

In this context, the aim of the reference model was to incorporate a
way of using the states of the participating agents to decide on an
appropriate mix of representations for the simulated entities, and the
mechanisms that support changes in the representation of these
entities. The maintenance of state for superceded representations
evolved from the basic strategy developed in \Ctwo.  Since the only
interactions in \Ctwo were between the simulated organisms and the
environmental plume, there was no need to include mechanisms for
agents to interact in a more general sense.

\section{Model implementation}

\ReModel is based on the the description of the example in Chapter
\Cthree. In the example, the domain consists of nine cells containing
tree-like plants. These trees are a critical food source for a
population of herbivores, and the trees rely on the young herbivores
to eat the ripe fruit in order to make their seeds viable (perhaps the
seeds have a particularly tough seedcase). A carnivore which preys
solely on juvenile herbivores is introduced into a stable system.
In \Cthree, the introduction of the carnivore and a subsequent
range-land fire initiated a sequence of events which, in turn,
engendered changes in the configuration of the model as a whole.  The
implementation described here is not identical to the system described
in the paper -- some inadequacies of the model presented in the paper
have been addressed (such as the inability of the plants to recolonise
a cell after they have become extinct in it).


\subsection{Scheme}
The design decisions for the modelling framework was heavily
influenced by my experience with large ecosystem models which
primarily used \Cpp and \CC.  While these models served their purposes
well and I incorporated a number of what turned out to be
``scheme-flavoured'' approaches in the handling of data, the
development of non-traditional components was made
difficult.\footnote{This was particularly well exemplified by the
problems associated with modelled entities (whales) which would leave
the domain of the model for much of the year, and during this period,
they would calve.}

\ReModel was implemented in the \Scheme programming language.
\Scheme was chosen because it has long possessed properties that are
only now becoming part of the standard in members of the \CC family,
such as anonymous functions, and first-class closures. Closures are a
natural way the data we want to preserve across transitions between
representations. The data is encapsulated in a maintenance
closure which holds and updates the data from a
representation which has been replaced. \Scheme was designed with a
minimalist approach and for exploring fundamental ideas about language
design, such as \texttt{call-with-current-continuation} (often
shortened to \texttt{call/cc}).  Scheme is a block-structured,
lexically scoped language, and while the preponderance of parentheses
(as well as brackets and braces in some dialects) can be daunting to
newcomers, its small size and simple syntax make it relatively quick
to learn.

I have specifically avoided the use of \texttt{call/cc}, arguably the
language's \emph{enfant terrible}. It is a control construct which is
an obvious avenue for implementing the multitasking system in the
model, but, somewhat like the bait in the trap, there are a number of
costs that make its use in a project of this nature unappealing: most
importantly, the behaviour of \texttt{call/cc} is not intuitively
obvious to most modellers and programmers, and it brings additional
complexity in debugging, and memory overheads with their associated
run-time costs.

A number of factors were important in the choice of implementation
language, namely
\begin{itemize}
\item[--] class-based multiple inheritance with polymorphism\\
\item[--] weak/dynamic typing\\
\item[--] linking with compiled code in other languages,\\
\item[--] able to support maintenance models\\
\item[--] potential for parallelising and distributed execution.\\

\listintertext*{and}
\item[--] interpreted operation if possible,
\end{itemize}

I considered \Cpp and \CC as implementation languages, since they
both\footnote{It isn't technically hard to construct a class system
for \CC, with the desired properties, but there is a reasonable amount
of effort, and the result would be idiosyncratic.} can address most of
these factors. I was disinclined to use other obvious candidates,
notably \Java, and \Csharp, and \DD do not support multiple
inheritance, and are strongly typed.

Using an interpreted language also meant that I could readily
incorporate both numeric quantities with physical units and functions
in the parameterisation of modelled entities in the parameter files
(\emph{c.f.}  the files for the
\method{mass-at-age} functions for animals and plants). \Scheme was,
at its inception, designed to be a tool for exploration
\cite{sussman1998first} and it remains an excellent tool for this
purpose. \Scheme is an actively developing language, and there are a
number of very good interpreters and compilers for it; many of these
implementations are also amenable to integration with other languages,
either through directly linking binary objects or by interacting
through a virtual machine (such as a JVM).

There are alternatives to using first-class closures to maintain
state-variables across representation changes, but these methods are
not opaque, and it is more difficult to ``protect'' the data from
accidental modification.  A closure is essentially a function which is
coupled with private state-data. It may take arguments, operate on its
state, and may return values, but no other code can directly modify
its state.  This feature means that an arbitrary representation of an
entity typically knows nothing about the internal features of the
maintenance closures it carries, and can only interact with them
according to the services they provide.

A simple example of a closure might look like so:
\begin{verbatim}
;; The closure will be called like (@ 'tick dt) 
(define @
  (let ((TBT 0) ;; tributyltin
        (depuration-rate 0.0001) 
      (lambda (action #!rest x)
       (cond
        ((eqv? action 'reset) (set! TBT 0))
        ((eqv? action 'value) TBT)
        ((and (eq? action 'tick) (= (length x) 1) 
           (number? (car x)))
         (set! TBT 
           (* TBT (- 1 (exp (* depuration-rate (cadr x)))))))
       (else 'ignored)
      )))))

\end{verbatim}
This example maintains a contaminant level in the absence of contact
with the contaminant, TBT.  Should the representation of the
modelled entity enter a situation where TBT is present, a monitor
class may decide that a representation change is necessary, and the
closure would be queried for the correct value for the contaminant
level to use in the construction of the new representation.

The variable \sttvar{TBT} is set to point to the function the
\Syntax{lambda} defines, and the state variable \sttvar{counter} is
``global'' to this function, rather like a static variable defined
outside a function in a C source file. Closures are a coupling of a
procedural element with data which is preserved across invocations of
the procedure. An agent maintaining this closure would be able to
fetch the value of TBT, cause the closure to run the depuration code
for a time step (\texttt{dt}) or reset the contaminant level to zero,
but nothing else.  More complex closures would typically be able to
provide a list of tags which specify external properties which are to
be used in their update step.

\subsubsection{Interpreter/Compiler}
I have used
\GambitC,\footnote{\URL{http://gambitscheme.org}} a well
regarded implementation of \Scheme which runs on all of the major
platforms (\Linux, \Unix, \OSX, \Android and \Windows). \Gambit
incorporates both a mature \Scheme interpreter and a compiler, and
both can dynamically link programs with external, compiled code and
libraries which may exist either as compiled \CC or \Scheme code.  The
choice to use \Gambit, rather than another version of \Scheme, was
heavily influenced by several points: it was the implementation used
for the model in \Ctwo; its ability to link to compiled \CC code and
its own compiled code can combine to produce programs which are
noticeably faster than purely interpreted code; it supports
very \CC-like infix notation (indeed, it can be programmed using
syntax which is almost-native \CC syntax); and adding hand-crafted \CC
or \Cpp code for particularly intensive routines is not difficult.

Porting \ReModel to other versions of \Scheme should be relatively
simple; the only potentially awkward issue in using a
different \Scheme implementation would be the use of \sdefmac in the
creation of the classes and methods in a model.  These macros are used
to automatically incorporate code which maintains data-structures that
provide information essential to the communication between submodels,
and the system as a whole. The macros are also able to detect and
respond to some sorts of oversights or inconsistencies in the
construction of methods or model-bodies.  In principle, \sdefsyntax or
other tools for syntactic extension or modification would be better
means of obtaining many of the effects of the macros defined in the
\fname{framework} file: \sdefmac was the basis for the first
incarnation of this framework, and -- at the time -- was the more
familiar mechanism, and I made the decision to persevere with it.

The use of \sdefmac complicates the process of locating syntax errors,
since the interpreter is unable to track the line numbers within a
macro. \footnote{The macros which wrap the methods called by an agent
are implemented in such a way that the code they produce can be
written to a file, and that file may be included in the place of the
original block of code.  In this way, the shortcomings with respect to
line number tracking can be ameliorated somewhat.  Such code could be
used to generate a code-base for a model which was free of macros.}
The only aspect associated with using macros which would require
broader changes with \sdefsyntax is synthesising the context-specific
identifiers (such as
\symb{<animal>-model-body}) which refer to the ``body'' of the
different classes of agents; this is not insurmountable, but will be
left for a subsequent version of the modelling framework.

\subsection{\SCLOS -- a \Scheme implementation of \CLOS}

The class structure in the \ReModel makes use of the \Scheme
implementation of \SCLOS that was written by Gregor
Kiczales \cite{kiczales1993xerox} while he was working at Xerox PARC
in 1992 and 1993.  Kiczales has been an instrumental researcher and
proponent for the use of meta-object protocols (MOPs) as a tool for
making computer programs clearer, more efficient, and more robust.
The basic tenet is that generic methods or functions are used to
manipulate objects, and that these generic objects inspect the nature
of the data being passed to them and pass the processing to a
specialised function of the same name which deals with the task most
appropriately.  While this is not an exact analogue to the problems we
seek to address in this work, there is a substantial similarity and
using \SCLOS with its implicit MOP seemed a natural fit.  \SCLOS has
been the basis for many of the significant object-oriented systems for
\Scheme.  In the development of \ReModel there have been some
instances where the ``wrong'' methods seemed to have been
called to act on an object, but these have inevitably been associated
with a type error in the signature of an overloaded method (in
the \SCLOS idiom, that would be a generic method with more than one
implementation), or a case where a generic method has been defined
twice. The modelling framework now protects against duplicate generic
methods. 

The basic \SCLOS library has been slightly modified
(the \mclass{<object>} class of \SCLOS has been renamed
to \mclass{primitive-object}) and additional support routines have
been added in the file \fname{sclos+extn.scm}.  These additions
include the recognition of a number of \Scheme data-types, routines to
examine the parents of an object, registers which recognise objects
and classes which have been instantiated, extended support for the
initialisation of slots in an \SCLOS object (parameter files), and
classification predicates.  

\subsection{Class structure}
The submodels in the example are all derived from a basic
\mclass{agent} class which provides each modelled entity the
facilities for interacting with the kernel of the model and mechanisms
for the agents to obtain information about the other agents and a
means of interaction.

The taxonomy of the classes is fairly conventional: the primitive
\magent is the superclass of almost all of the model components
(exceptions include classes for things like bounding regions), and it
provides submodels with access to the infrastructure of the system. 

At the lowest levels, the classes provide definitive features that
characterise broad categories of entities, such as discrete locations,
delineated areas, the ability to map between ordinate systems, or the
ability to poll other agents at regular intervals.

\begin{table}[H]
\begin{center}
  \caption{Fundamental classes in the \ReModel -- \fname{framework-classes.scm}\label{classtableI}}
  \begin{tabular}{l L{7cm}}
    \toprule 
    \textbf{Classname} & \textbf{role} \\
    \midrule
    \mclass{agent} & The agent class provides the fundamental
    components for a model to run within the framework, such as
    essential state variables and common methods \\
    \mclass{projection} & This class is derived from \mclass{object};
    it adds methods and state variables to allow mapping between an
    agent's native coordinate system and the common coordinate system.
    It extensively used by the routines that output
    graphical data -- mapping each agent from its domain to the domain
    of the output page or image.\\
    \mclass{introspection-agent} & The methods which allow
    \mclass{monitor} and \mclass{log-introspection} agents to operate is provided by
    this class -- it forms the basis for agents which poll other agents \\
    \mclass{plottable} & Children of this class can be plotted in
    map-like output. The attributes of this class include a ``glyph''
    a location, an orientation, a preferred font, colour, glyph size
    (which may be scaled by the value of a slot) and an additional
    magnification which can be used to adjust the scale.
    This is the class that provides location and orientation to the
    animal and plant classes.\\
    \mclass{environment} & An agent's basic spatial context is provided by
    this class's bounding box and a link to an explicit representation\\
    \bottomrule
  \end{tabular}
\end{center}
\end{table}
The classes in Table \ref{classtableI} provide the basic mechanisms
essential to agents of each branch.  They provide a means of
communication with the kernel of the framework, and mappings between
the internal and external representations of agent's spatial domains.

\begin{table}[H]
\begin{center}
  \caption{More fundamental classes -- \fname{framework-classes.scm}\label{classtableII}}
  \begin{tabular}{l L{7cm}}
    \toprule 
    \textbf{Classname} & \textbf{role} \\
    \midrule
    \mclass{object} & This is the ``primal'' class for all of the
      different agent classes, and the more complex data structures
        like polygons  \\
    \mclass{array} & Arrays are used to construct a collection of
    lists which often act as the basis for a model representation
    between a fully defined individual-based representation and an
    analytic representation.  Lacks the potential temporal sensitivity
    of an individual-based representation, but able to maintain many
    i-state variables (\textit{sensu}\/\cite{caswell1992individual})\\
    \mclass{diffeq-system} & implements a class which runs a
    Runge-Kutta4 algorithm for a system of differential equations.  This
    class doesn't maintain variables as such, rather it is passed
    ``getters'' and ``setters'' which obtain state variable values from
    other agents (such as \mservice agents) and subsequently
    set new values in those agents.\\
    \mclass{proxy} & Proxies represent some ``element'' (like a row in an
    \mclass{array} agent) when engaging in individual-to-individual
    interactions.\\
    \mclass{model-maintenance} & provides the support for maintaining
    state variables for representations which are not currently active
    (such as in \Ctwo).\\
    \bottomrule
  \end{tabular}
\end{center}
\end{table}


\subsubsection{Introspection agents}
The \mclass{introspection} class is geared to to supervising and
extracting data from the set of running agents.  The classes of agents
(listed in Table \ref{classtableII}) are derived directly or
indirectly from \mclass{introspection} and the classes \mclass{logger}
and \mclass{monitor} classes exist solely to poll the other agents
within the system and, respectively, either extract information in an
analogue of the sampling undertaken by researchers, or to monitor the
states of agents, ensembles and the model as a whole in order to be
able to effect changes in the constituents of the model ensemble when
necessary. The loggers handle all of the generation of output files --
the code which makes up the submodels only provides one or
more \method{log-data} methods which the loggers use to assemble their
output.  The notion is that the agents do not need to know about what
is being sampled, only how to make the data they can provide
``presentable'' and the loggers don't really need to know the details
of a submodel's workings, only that it ought to obtain certain data
from agents of particular types and generate an output file.

While this approach to data output engenders some extra overhead, it
has a number of advantages which are not necessarily
obvious. Consolidating the output into separate agents makes changes
to the format of the generated output a simple matter of changing
which output agent is used, or which format is required. The same
mechanism, or one which is very similar, could be used as the basis
for coupling distributed models.

Using \mlogger agents rather than embedded output code also means that
we can confine code associated with generating ``output'' (mainly
the \method{log-data} methods) to a very small section of a few basic
parent classes; thus instances of all the child classes can
automatically be queried without the necessity of writing new output
routines.  The MOP of \SCLOS is used to map calls from the
introspection agent to the appropriate version of \method{log-data},
and the chain of inheritance can be used to call the \method{log-data}
methods of its parents to structure the output appropriately.

In many ways, this is a direct analogue of field sampling, and
designing the mechanism in this way makes the direct simulation of
field sampling quite straightforward.

\begin{table}[H]
\begin{center}
  \caption{Introspection classes in \ReModel -- \fname{introspection-classes.scm}\label{classtableII}}
  \begin{tabular}{l L{7cm}}
    \toprule
    \textbf{Classname} & \textbf{role} \\
    \midrule
    \mclass{log-introspection} & The fundamental mechanics that
      underpin the regular polling of other agents in the system\\
    \mclass{logger} & {provides the basic structure for generating model output}\\
    \mclass{monitor} & {provides the machinery for assessing the state
      of the system and effecting changes in its configuration}\\
    \bottomrule
  \end{tabular}
\end{center}
\end{table}
The introspection classes are defined in the files
\fname{introspection-classes.scm}, \fname{monitor-classes.scm}, and
\fname{log-classes.scm}.  Of these, \fname{log-classes.scm} is the
most complex.

The monitor classes are able to maintain a history of the decisions
they make and can, in principle, factor this history into the
decisions they make subsequently.  As yet, the \mmonitor classes are
still quite simple, acting to coerce changes in the constitution of
the model ensemble according to fairly simple rules.

\goodbreak
\subsubsection{Agents with locations}

The individual-based classes are all derived from a small ensemble of
classes which endow them with the physical attributes we expect, such
as location, and mass. All of the biotic agents in this branch of
classes are able to maintain a history of their movement through the
model's geographic domain and may also maintain a record of state
variables. The agents may inspect these data and potentially use them
in their own decision-making process; thus, some (as yet
unimplemented) model of a animal may ``recall'' a recent encounter
with a predator in a location and avoid the area.

\begin{table}[H]
\begin{center}
  \caption{Basic individual-based representations
  -- \fname{framework-classes.scm}\label{classtableIII}} 
  \begin{tabular}{l L{7cm}}
    \toprule 
    \textbf{Classname} & \textbf{role} \\
    \midrule
    \mclass{tracked-agent} & Agents derived from this class automatically
    maintain a time-stamped history of where they have been. This data
    can be extracted for analysis or plotting.\\
    \mclass{thing} & adds mass \\
    \mclass{living-thing} & This class adds age and mortality, also aware of the
    environmental context in which it lives \\
    \mclass{plant} & The general structure for plants is provide here.
    The more refined \mclass{example-plant} is derived from \mclass{plant}.\\
    \mclass{simple-animal} & Instances of this class make use of a
  simple metabolism, simple modal behaviour\\
  \mclass{animal} & extends the behaviour of simple animal by making
  the metabolism and modal behaviour more complex\
  \mclass{example-animal} This class adds reproduction, density
  limits, and the ability to migrate when conditions warrant it. The
  classes used to represent the animals in \ReModel are
  derived from this class.\\
    \bottomrule
\end{tabular}
\end{center}
\end{table}

The classes for the simple organisms of \Cthree are defined in the
files \fname{animal-classes.scm}, \fname{plant-classes.scm}, with
corresponding ``\fname{\ldots-method.scm}'' files with the
implementation of the methods.

%%%% plant classes

Plants and animals are both implemented in classes derived
from \mclass{living-thing}. An analytic representation for plants
based on \mclass{diffeq-system} is also implemented.

There are essentially three implementations for modelling plants in
\ReModel. \mclass<plant> is the basic starting point for
the \mclass<example-plant>, which implements a model substantially
similar to that described in
Section \ref{adaptiveselection}.\ref{plantch3}. The alternative
representation for plants is the \mclass{plant-array} which is an
array of state-vectors which apply update methods to each of the state
variables which are similar to those used for the individual based
version. \mclass{plant-proxy} is an interlocutor for
a \mclass{plant-array}, though we have not employed it in this
model.\\
%%%% animal classes

The treatment in the animal classes is similar to that of the plant
classes. Both the \mclass{aherb} and \mclass{jherb} can be represented
by \mclass{animal-array} agents, but we refrain from carrying the
generalisation to include the \mclass{carnivore} class -- carnivores
are unlikely to reach high population levels, and their behaviour is
more explicitly discrete.


\subsubsection{Agents associated with areas}
Most of the entities which are associated with an area rather than a
specific location are members of the environmental classes defined
in \fname{landscape-classes.scm}.  Exceptions include equation-based
representations for populations which may be associated with
particular \mclass{environment} agents.

\begin{table}[H]
\begin{center}
\caption{Non-spatial environments elements -- \fname{landscape-classes.scm}\label{classtableIVa}}
\begin{tabular}{l L{7cm}}
\toprule
\textbf{Classname} & \textbf{role} \\
\midrule
\mclass{ecoservice} & Usually associated with a patch, this class
provides a data value which can be inspected and modified by other
agents.  It is possible for the ecoservice to access data in other
agents, and to have implicit growth or decay associated with its
value.  Instances are able to store the (time value) pairs in a
history list.\\
\mclass{population-system} & This is a refined version of
the \mclass{ecoservice} which primarily provides a simple ``biomass''
agent.\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

The classes in Table \ref{classtableIVa} are typically incorporated
into environments associated with nominated areas, though they need
not be.  Both ecoservices and population systems can play the role of
spatially agnostic agents which are pertinent for the whole model
domain, such as external forcing conditions. In contrast the classes
in Table \ref{classtableIVb} are explicitly associated with bounded
regions currently represented by circles or arbitrary polygons, or
sets of these bounded regions.  

\begin{table}[H]
\begin{center}
\caption{Spatial environments -- \fname{landscape-classes.scm}\label{classtableIVb}}
\begin{tabular}{l L{7cm}}
\toprule
\textbf{Classname} & \textbf{role} \\
\midrule
\mclass{patch} & The basic geographic region is implemented in
\mclass{patch}. It has a bounded area,  a list of ecoservices which
are assumed to be uniformly available, a ``notepad'' which can be
interrogated for state information, and it may also have ``caretaker''
routines defined in parameter files which get executed every time the
patch gets a time step.\\
\mclass{dynamic-patch} & Patches  in this class may have a system of
differential equations which stand in the place of the simpler --
possibly simplistic -- dynamics possible using only the ecoservice
mechanisms.\\
\mclass{landscape} & An instance of this class maintains a list of
patches and a terrain function which provides an altitude ordinate to
the locations in the geographic domain; in aquatic/marine models, this
would typically be used to provide the depth of water at a give point.\\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\subsection{Parameterisation}

Model agents (instances of the above classes) are made by calling the
\method{create} function.  The principal tasks of \method{create} are
to allocate storage, and to call the functions which initialise the
state variables of an agent.  \method{create} initialises its state 
variables using the data from the files in a nominated parameter
directory. The state variable initialisations specified in
class--based files are applied first from the parent class farthest
away in inheritance, through to file associated with class of the
agent being created. It is quite likely that a given state variable
may be reset several times in the process as the model representation
is refined. Finally, the state variable settings from a file
associated with a \emph{particular}\/ set of instantiations -- the
agent's taxon -- are applied.  A model might deal with several
populations of distinct species (taxa) of tuna.  These taxa may have
different prey preferences, temperature preferences, and metabolic
parameters.  Each distinct species would load its initialisation
parameters from a taxon file associated with the species.

In order for the initialisations indicated in these files to be
applied in the correct order, class--associated files must have the
same name as the class they relate to (such as \fname{<tuna>} and
\mclass{tuna}), and taxon-associated files will have the same name
(including case) as the taxon used (such as
\fname{T.albacares} and \strng{T.albacares}).\footnote{It doesn't matter if there are
  entries for things which are not state variables -- any specification
  which isn't recognised is silently ignored.}

Parameter files contain \Scheme code and they are loaded
immediately after all of the code associated with implementing
representations of submodels has been loaded, but before any model
initialisation occurs. 

The initialisation data for agents is primarily taken from files in a
nominated directory (in our case ``\fname{./parameters/}).  Each file
is associated with either a particular class, or with a particular
taxon.  When agents are created, the instantiation process first
initialises all the state variables to \symb{<undefined>} -- a value
used to identify an uninitialised value.  The initialisation then
applies any initialisations which may be found in the file associated
with the \mclass{agent} class, namely \fname{./parameters/<agent>}.
The process continues, from most general to most specific, till any 
initialisations found in the agent's parent classes have been
applied.  The system then looks for a ``taxon'' file which which
contains very specific initialisation data -- we might have a generic
\mclass{quercus} class, for example, which serves adequately for both the
\fname{Q.robur} and for the \fname{Q.petraea} taxa, but the species
specific differences are assigned last, and take precedence.  Finally any
initialisation indications found in the \method{(create \ldots)} call
used to instantiate the agent are applied.  In this way, more specific
parameters (like metabolic rates) get applied later in the process.

A typical parameter file might look
like look like the '\fname{<example-plant>}' file below:
\label{parameters}\label{kdebug}
\begin{verbatim}
'Parameters
(kdebug '(loading-parameters 
          taxon-parameters "B.ex-longevity" "*.ex-*") 

(define B.ex-longevity (* 37 years))
(define B.ex-mass-max (* 150 kg))
(define B.ex-mass  (rk4 (lambda (t y) ;; make the d.e.
       (* 5e-7 (- 1 (/ y B.ex-mass-max)))) 0
   B.ex-longevity (* 4 weeks) 0.1)) ;; stepsize initial-val

(define <example-plant>-parameters
  (list (list 'cell '<uninitialized>)
        (list 'peak-mass 12)
                 ;; this will seed trees in the
                 ;; 4km x 4km square around
                 ;; the origin
        (list 'location 
          '($ (random-location (list -2000 -2000) (list 2000 2000))))
        (list 'mass '(\$ (nrnd 4 1 0.01 12)))
        (list 'height '(\$ (* kg (min 12 (+ 12 (nrnd 10))))))
        (list 'fruiting-mass (* 8 kg))
        (list 'fruiting-prob (/ 0.1 week)) ;; 10% per week
    (list 'fruiting-rate (* 7 (/ 1 kg))) ;; N / (kg week)
    (list 'mort-prob (/ 0.1 year))
    (list 'seeds-per-fruit '(\$ (+ 4 (urnd 8))))
    (list 'mass-at-age B.ex-mass)
    (list 'references  '(J. Muppet Botany, S.Chef et. al. v2,1995))
  )
)
(set! global-parameter-alist 
   (cons (cons <example-plant> <example-plant>-parameters) 
         global-parameter-alist))
\end{verbatim}

The \symb{'Parameter} line which is the first entry in the file is
an indicator that this is a parameter file, and the model 
uses this to determine that the file is a valid parameter file.
The definition which follows is a list of lists where the first
element of each of the second level lists must be a symbol.  These
symbols should (but are not required to) correspond to the state
variables in the agent. Note that even though this file is associated
with \mclass{example-plant}, it is not restricted to setting state
variables defined in \mclass{example-plant} -- it is \emph{both reasonable
  and common} to set variables associated with parent classes, such as
the plant's \sttvar{location} in the example above.

The last three lines of the file add the definitions to the list of
all the state variable defaults the system knows about -- if they are
missing or compromised, the definitions may not be accessible.

The parameter files from which the parameterisation is taken must be valid
\Scheme code, and as such can contain variable and function definitions.  The
example above includes the function definition for the mass-at-age
function for the taxon \strng{B.exemplarii}.  Na\"ively we might
assume that we would be able to perturb the starting mass of each
individual tree by just incorporating a random number into the mass
indicated in the parameter file: we can do so, but the parameter files
are loaded early in the loading process, before any agents are
actually instantiated.  At the point we begin to create agents, the
parameterisation data is static.  In order to overcome this, the code
which accesses parameters recognises that parameters of the form
\symb{(\$ \ldots )} should be evaluated dynamically when they are
accessed and passed back, so the line

\begin{verbatim}
   (list 'height '(\$ (* kg (min 12 (+ 12 (nrnd 10))))))),
\end{verbatim}

will return a value greater than twelve, with a distribution of twelve
added to the left half of a normally distributed variable with standard deviation
of ten.  Other examples can be found in the parameter files with
\fname{carnivore}, \fname{adult-herbivore},
\fname{juvenile-herbivore} and , \fname{B.exemplarii}. 
It should be emphasised that the parameters obtained from the
parameter files may be overridden in the \method{create} call, and by
code within the model itself.

In general, units are specified in the parameter
files, and should also be specified when indicating particular
initialisations for agents.  The (extensible) list of known units is
found in the file \fname{units.scm}.


\subsection{Model initialisation}

The first stage in a model run is the creation and initialisation of the
components of the model. A model is constructed by a (comparatively)
short \Scheme program which uses the infrastructure to create
and configure the agents which comprise the model ensemble.  These
agents are added to a list called the \variable{runqueue}, which
corresponds to the execution queue in a simple multitasking operating
system. The agents are created by a call to a routine in
\fname{sclos+extn.scm} that looks like
\begin{verbatim}
    (create <landscape> taxon 'name "domain" ...)
\end{verbatim}
where the ellipsis indicates additional state variable
assignments which supercede any previously set values from the
parameter files. Agents which have an associated region or location
would usually require some additional code to to add agents to
encompassing entities (such as including trees in the landscape) this
would often be a part of the code which creates the
agents\footnote{See the file \fname{specific-model.scm}, or spawning
  code in \fname{animal-methods.scm} for an illustration of this.},
or would be effected in the class-method \method{initialise-instance}
which is called as a part of the process that prepares an agent to be
run; generally, initialisation of an agent would be accomplished using
both of these approaches.


\subsection{Methods, model bodies and closures}

Model methods are essentially functions that are explicitly associated
with the class signature of the arguments passed to them. For example,
there are several versions of the model method \method{dump}, and each
is made distinct by the class of its arguments, particularly the first
argument, and each of the methods may be invoked only by members of
the appropriate class or of its descendants.  This ability to have
several functions with the same name is made possible by the use of
instances of \mclass{generic-methods} which maintain an internal list
of actual functions and information about the types of the arguments
they expect. When a call to a method is made, it is the generic-method
that receives it, and it examines the arguments passed in order to
determine which of the \Syntax{model-methods} should handle the call.
The call is then forwarded to the appropriate actual implementation.
This means that \mclass{shark}and \mclass{seal} may both have an
\method{eat} method -- moreover, \mclass{shark} may have two
\method{eat} methods: one which recognises when the item being eaten
is an \mclass{animal}, and one where it is merely an \mclass{object},
which would usually be inedible.  This multiple-dispatch means that no
explicit test is required in the model code.  In this case it would be
worth worth writing an \mclass{object} version which causes the agent
to spit out the offending item.  If there was only an version of
the \method{eat} method which implemented eating \mclass{fish},
for \mclass{shark} agents, they will only attempt to eat fish, and
presenting them with anything else will raise an error.  This ability
to catch and recover from unexpected arguments makes
the extension of classes much less vulnerable to undetected error
conditions. 

There are two golden rules to \Syntax{model-methods}
\begin{description}
  \item[Only define each generic method once] Redeclaring a
    generic-method or \Syntax{define}-ing a method
    (\Syntax{define} \method{hunt} (\method{generic-method})) destroys
    any previously declared \method{hunt} methods, generic or
    otherwise! To this end, all declarations of generic methods must
    be made in the \fname{framework-declarations.scm} file, which
    should contain the declarations of the all generic methods used by
    the model, apart from a very special few declared in \fname{sclos+extn.scm}.
    Related to this is the problem of defining a method
    before you have a generic-method declared -- the macros
    in \fname{framework} should catch this.
    \item[Don't construct methods that match the same argument lists]
    Generally, \SCLOS is very good at getting the right method for the
    job, but there are situations where it can be confusing for it.
    If you have a situation where there are several possible desired
    code paths for essentially similar arguments, it is clearer to
    have an explicit selection made within the body of a single
    method, and that way is much easier to debug.
\end{description}
 
Model bodies are special methods which are only called by either the
kernel or by the subsidiary execution lists embedded in agents. There
is no provision for an agent to directly call model bodies.  Each
agent's \Syntax{model-body} is run by the kernel at each of the
agent's time steps; there is no \Syntax{object-body} since objects are
(by design) not able to be run by the kernel.

\section{Execution and Control flow}

After the initial cohort of agents have been instantiated and
introduced into the \symb{runqueue} (or in the
\sttvar{subsidiary-runqueue}s of other agents), control is passed to
the kernel which then begins to run each agent in turn for an
appropriate time step. The queue is ordered first by the
\sttvar{subjective-times} of the agents, then their relative precedence, and
finally an optional \sttvar{jiggle} value which can be added to ensure that
there are no systematic preferences within a time step.

The most important routines in maintaining an orderly flow of control
from one agent to the next are
\begin{itemize}
\item[\method{run-agent}] which manages the interactions between the
  agent and the \variable{runqueue} and constructs the procedure that the agent uses
  to communicate with the kernel.

\item[\method{run}] calculates the upper limit on the amount of time the
  agent will run, runs the body of the model (declared in a
  \Syntax{model-body} block), runs any nested agents and attends to 
  the maintenance of any data from superceded representations.
\end{itemize}

The sorted queue of agents is maintained by the system; the agent at
the head of this list is removed from the list and execution is passed
from a call to \method{run-agent} to \method{run}  which then passes
control to the agent's \Syntax{model-body}. The call chain is split in
this way to separate interactions with the run queue and the
management of the activities of agents.  The motivation for this
separation is that provides an avenue for changing the nature of the
system used to run the models, say from a single-threaded execution
list to a number of parallel lists.

The \Syntax{model-body} of an agent takes control with an indicated
maximum amount of time over which it might run.  Agents need not run
their whole time step -- events may occur which cause them to truncate
their turn, returning control to the kernel with an indication of the
amount of time they actually used.  When the kernel receives control
from an agent, it examines the data passed to it by the agent and acts
accordingly: terminated agents may be silently dropped from the queue,
for example, and other agents may be reinserted in an appropriate
place in the queue for the their next time step, or agents
representing organisms which have just reproduced may introduce new
agents into the \symb{runqueue}.

Control also passes from agents to the kernel when the agent makes
queries looking for prey or to interact with other agents: a carnivore
might want to know whether there are any prey animals within a certain
radius, for example.  Here, a request would go to the kernel for a
list of nearby prey.  The kernel would then examine the list of agents
meeting the requirements (type, spatial location, temporal contiguity)
and pass the list -- and control -- back to the hunter.  This is a
common paradigm in both agent-based modelling and in
computer-operating systems design, where access to information or
resources are obtained from a delegated ``authority''.

The maintenance of state data from a superceded representation is
rather different. When an agent is given data to maintain, it
processes them at the end of each of its time steps. Each is asked each
what set of data their maintenance closure needs in the update step;
the set of data is obtained control and the data to run the maintenance
closure. When the closure has finished its update, control returns to
the agent, and control ultimately passes back to the kernel.


\section{Interaction with the kernel and other agents}
\subsection{Calls to the kernel}\label{interactions}
The first step in an interaction between agents is often to find other
agents which fit particular criteria: a predator looks for nearby prey
or a travelling salesman might look for accommodation within their
budget, for example.  Many of these searches are conducted either by
using one of the \method{locate} calls provided by the kernel, or by
taking advantage of previously cached information maintained by the
searcher. The searches can be restricted to particular spatial
regions, particular sets of taxa (as determined by the agent's
\sttvar{taxon}), particular classes, or the possession of particular
state variables (such as \sttvar{location} or
\sttvar{available-rooms}). The nature of the interactions may range from
merely ascertaining the presence of another agent, to predation, or
extracting state information (such as mass, location or the price of a
room).  A typical call to the kernel would look like
\begin{verbatim}
   (set! target (kernel 'locate (apply *provides-*? (my 'requires))))
   (set! vtargets (kernel 'providers? (my 'requires)))
\end{verbatim}
Both \variable{targets} and \variable{vtargets} should have the same
values.  There are a number of predicates defined
in \fname{slos+extn.scm} which are similar to \method{*provides-*?}
which select on class, taxon, location, possession of particular state
variables, the roles an agent may play, or combinations there-of.

\subsection{Spatial queries}
Agents that regularly interact with other agents which are close to
them in the spatial domain will often cache references to their
important neighbours to reduce the overhead of calls to the kernel
routines, since direct communication with target agents has a much
lower overhead. Direct contact does require agents to check to see
whether the agent being queried is active or has been replaced by
another representation, since both state and representation changes
can occur without the agent knowing. Most state changes of an agent
will be from \symbolic{alive} to \symbolic{dead}
or \symbolic{terminated}: the first two indicate that the agent is
still available for interaction, the third indicates that it is no
longer a participant in the system. Should the object of an agents
interest become \symbolic{terminated}, the agent can then query the
terminated agent to see if it has been replaced by another
representation; if so, it may be able to obtain a reference to the new
instance directly from the superceded agent, or query the kernel for
the reference.

\Scheme only reclaims objects (in this case instances of a submodel)
when all references to the object have vanished; this means that as
long as an agent maintains a reference to the entities it is
interested in, those entities will persist, and can be used to obtain
a reference to instances which have replaced it.

Agents representing animals or plants will typically already know
about their ``domain'' -- the landscape they inhabit.  Landscapes are
usually cast as bounded geographic regions associated with
environmental data such as water availability, topography or
contaminant levels. In contrast, an animal would typically \emph{not}
keep track of other animals.  The agents in \ReModel only
cache a limited amount of information in this regard.

The basic calls which return the locations associated with agents look
like
\begin{verbatim}
...
   (let* ((prey (kernel 'locate 
                  (apply *provides-*? (my 'food-list))))
          (cover  (look-for self (my 'prey-hides)) ) ;; another way of querying
            ;; plants that can hide my prey
          (preylocations 
            (map (lambda (x) 
                   (query x self 'location 
                           (my 'search-radius))) 
                 (append prey cover)))
         )
         ...
)
...
\end{verbatim}
In this example, the first of the three clauses obtains a list of
candidate prey with a call to the kernel\footnote{In this case
  \symb{locate} returns all possible candidates irrespective of
  location, but insists that the targets are
  contemporaneous. \symb{locate*}, in contrast, does not enforce
  temporal consistency. Both are able to filter the results based on
  location, class or the role targets play in the simulation.}  The
next searches for potential hiding places with the \method{look-for*}
call. This call does not enforce synchrony\footnote{The alternative
  \method{look-for} does enforce synchrony.}, and the last actually
extracts targets' locations for subsequent use.

Once an agent is aware of another agent, it will typically interact
using either the \method{query} call or calls to the methods
appropriate to the target. \footnote{The MOP paradigm makes it possible to use
substantially the same code for many agents of many classes, but calls
through \method{query} may be easier to convert for parallelism later.}


\section{Introspection agents: loggers and monitors}

There are classes which are primarily structured to periodically pass
through a list of running agents.  Instances of these classes have the
ability to glean data from the agents they query, cause the agents to
change their behaviour, and even remove or replace agents.  The
classes with these special properties are all derived from the
general \mclass{introspection} class.  The most important role they
play is in the generation of the model output, but they are also
responsible for the ability of the model to adaptively change the
representations used in is simulation of a system.


\subsection{Generating output files -- loggers}

The \mclass{log} agents do their work by first generating a list of
agents to be polled, and then calling a routine to ``emit a page'',
which really just means emit the data which is appropriate for this
iteration.  This task has three phases: first it calls the
method \method{page-preamble} which prepares state variables for
processing the appropriate sort of assessment (agent-level,
taxon-level niche-level, or configuration-wide), then it iterates
through the list of targets provided by the kernel apply
the \method{log-data} method, and finally it
calls \method{page-epilogue} to finish generating the output.  It
should be noted that \emph{only\/} \mclass{log} has
an \method{emit-page} method -- this method works for all loggers,
since they are derived from \mclass{log} and the class-specific
specific details are devolved to the children.

The method\method{log-data} is used because the child classes often
have significant differences in their state variables, their internal
data structures and output formats.  Dividing the processing in this
way means that data can be accumulated in an orderly way and either
written or cached until it is suitable for writing.  Each of the
classes that can be logged must either use the
\mclass{log} version of
default \method{log-data} (associated with one of their parents
classes) or supply its own.  Some of the leaf classes produce part of
their output, but also chain back to the parent \method{log-data} for
part of their processing.

As mentioned before, the decoupling of the models and the generation
of their output is not without some overhead, but the net gain -- at
least in this model -- has been worthwhile. Not only is starting the
logging of data after the model's ``spin-up'' simple, but the loggers
can be configured to replicate different field studies without
requiring modification of the underlying model.

\subsection{Changing representations}

Monitors are very similar to the logging agents and use essentially
the same infrastructure for agent selection and processing.  The
process of assessment and the management of the representations of the
systems being modelled is reasonably simple to describe: periodically
the monitoring agent will interrogate the subset of the model for
which it is responsible. Each time it does so, it generates a tree or
set of trees which encode the state of the model and the system's
assessment of their merit with respect to the location of the model
(or components) in the state space.  These periodic assessments are
essentially ``maps'' of the current configuration of the model into
the metric-space of the trees of \Cfour.  We use this mapping to
calculate the closest candidate configuration from a set of
configurations we believe (or know) to represent the system well in a
given part of its state space. Because the trees are elements of a
ring, we can, in some cases, interpolate between configurations,
arriving at possibly advantageous configurations which have not been
explicitly specified.

There is an overhead to using monitors to adjust the configuration,
like loggers, they must run periodically and poll agents. While
branching within a model is undoubtedly a lower cost, the tests
attendent to both will be substantially similar, and by shifting the
business of changing representation to an appropriate monitor, all
representations for an entity automatically become capable of making
that change without altering their code. 

The processing loop for a \mclass{monitor} agent has a number of
steps:
\begin{itemize}
\item[---] Each of the agents will be queried for its status
and its own assessment of its state in the context of the other
elements it interacts with.  This may be fairly trivial for most
agents, particularly those which change slowly, or are not
significantly influenced by the activities of other agents.
In the majority of cases, the agents will return a list with a
self-assessment of their current levels of fidelity and cost.
\item[---] The monitor synthesises a number of representative
state-trees which describe the status of the configuration of agents
by taxon and niche, and it generates a state-tree which describes the
current configuration. Other factors dealing with significant
conditions which pertain to the simulation -- perhaps relating the
geographic distributions, population numbers or density, the abundance
of resources, or the presence of significant conditions such as fire,
may be incorporated into the configuration trees and the candidate
configuration trees.
\item[---] The monitor then compares the current state against
the \emph{known-good} and \emph{known-bad} configurations, and records
these values.  Each of the generated status trees is then compared
with appropriate \emph{good} and \emph{bad} trees, and these data are
used to select the best candidates for consideration.  The trees are
constructed so that we can basically add the trees that represent
agents or groupings of agents to construct a tree that represents a
model. Each of these synthetic trees is ranked and, if there is enough
benefit, the desirability of a change is indicated.
\end{itemize}

The assessment trees are then used to direct the conversion of groups
of agents to other representations.  Sometimes this will entail the
collapsing of a number of individual-based agents into a
super-individual, an equation-based representations or possibly a
hybrid of some sort.  Other times, a more aggregated entity may be
converted to a more discrete form. The strategies for conversion
depend on the nature of the representations in question, and this is
probably the most challenging part of constructing a model of this
kind -- in order to maximise its utility, the conversions should be
carefully considered.  One of the benefits of this type of examination
is that the sensitivities of the model must be explicitly analysed and
dealt with.

Developing a body of configurations which function well is likely to
be, at least initially, a process of intuitive selection and trial and
error. Initially, we begin with coarse knowledge regarding the utility
of particular representations under given conditions. \mloggers can
collect and record information about the agents within the simulation,
and their performance. This data can be used as the basis for the
selection and testing of new configurations, and we can classify them
into varying degrees of ``good'' and ``bad'' configurations.

This process depends on being able to discriminate between
configurations, and we do this by mapping configurations into
state-trees which are constructed in a way which encodes coarse
spatial information (the relationship between \mpatches, for example),
the numbers of entities represented by different classes, and their
own performance measures.

The replacement of an agent by another is accomplished by a routine
using the \method{create} function as used in
the \fname{specific-model.scm} file. There is an issue associated with
the risk of exhausting the available memory, however: if any memory
reference in an active memory allocation refers to a scheme object
then the object is protected from garbage-collection.  This means that
update-closures which are created with a built-in reference to the
creating agent protect that agent from having its memory returned to
the pool.\footnote{This sort of issue is easy to recognise -- the
computer rapidly becomes sluggish or almost completely unresponsive
because it is spending all its time in swap space.}

\subsubsection{Maintaining state across representations}

When an entity is being prepared to change its representation, its
current representation may create an encapsulation of its critical
state variables in a closure that is bound to a function which is able
to access these variables, and to maintain their values. The closure
may ultimately pass these values back to another agent which will take
over the role played by the model maintenance function.

A typical closure definition might look like
\begin{verbatim}
(update-closure <plant>-B.exemplarii:14 
    '(age mass taxon location)
    (list ;; initialise with these values
       (slot-ref self 'age)
       (slot-ref self 'mass)
       (slot-ref self 'taxon)
       (slot-ref self 'location))
    (list ;; 
       (lambda (t dt a m t l) (set! age (+ age dt)))
       (lambda (t dt a m t l) (set! mass (mass-at-age (+ age dt))))
       #f
       #f))

\end{verbatim}
In this example the state-variables \variable{age} and \variable{mass}
are updated in the body.  Closures are (at least in this model) unable
to access anything other than the arguments passed to them by the
model responsible for maintaining them.  The \method{update-closure}
block is implemented as a macro that is turned into a functional
closure with bound variables.  In order to interact with the closure,
it is passed one of the
symbols \symbolic{variables}, \symbolic{values},
\symbolic{id}, \symbolic{update}, \symbolic{quit} or the
\symbolic{update} followed by the values for the variables
listed in response to the \symbolic{variables} in that order.  The
values \variable{t} and \variable{dt} are always passed in by the
system.  When a \symbolic{quit} is issued, the values of the variables
are returned with their symbolic tags in an association list.


%\subsection{Mapping from interpolated configurations to actual
%  configurations}\HERE{Unfinished business}

%% \textbf{CLARIFY THIS}
%% Where closures are \emph{supposed} to be bound to communicate with
%% only the agent maintaining them, other entities may have other, more
%% direct, channels. Chief amongst these are the connections between
%% subsidiary agents and their containing supervisor.  These agents can
%% use class methods to query each other directly. The constraint imposed
%% on maintenance closures exists primarily because they really only
%% exist \emph{in potentia}, and the data they request may actually be
%% synthesised by the agent responsible for their maintenance, rather
%% than the agents which existed at the time the closure was created.

%% This is an uncomplicated system, more significant issues would arise
%% in a parallelised system.  

\subsection{Comparison of states}
The first step in being able to quantitatively compare configurations
of models is to construct a means for encoding the state in a way that
supports some sort of qualitative or quantitative comparison.  The
trees described briefly in \Cthree and more comprehensively in \Cfour
are elements of a metric space. Since the set of indeterminate
variables in the labels of these trees is quite arbitrary, we can
identify any particular attributes of interest in a family of
representations with with indeterminates, and we can also extend this
to denoting the various representations for the modelled entities.

The framework maps the configuration of the model to a tree in the
metric space defined in \Cfour as a means in order to allow us to
calculate how close one configuration is to another.  The example tree
(below) consists of only a few agents, but it illustrates the
type of structure and information used in determining the relative
merits of different configurations. Only three taxa are represented in
this tree -- \emph{fruit}, \emph{seeds} and \emph{B.exemplarii}, and
only the plants are represented by more than one type of agent.
The tree includes in ``leafmost'' nodes the identifying information
that ties specific agents to particular branches of the tree.  In this
way, we can apply a $\trim$ operation (Def. \ref{trim}) and calculate
the distance between representations without the details of specific
members of the aggregate data interfering in the result.

In this example, the numbers in the ``weight'' position, after the
colon, refer either to the aggregate values (in the case of
equation-based ecoservices), the numbers of distinct agents, or the
serial numbers of the specific agents in leafmost nodes.  A number of
useful indicators can be constructed from the simple operations
defined at the begining of \Cfour.

\begin{verbatim}\label{tree1}
(taxon : 0 {
     (fruit : 2792746.800173795 
      {(fruit + eqn-based + <ecoservice> : 625055.9572307098 
          {(gridcell-1,2:fruit : 21  {})})
         (fruit + eqn-based + <ecoservice> : 132324.34236008758 
          {(gridcell-1,1:fruit : 19  {})})
         (fruit + eqn-based + <ecoservice> : 362001.18511443434 
          {(gridcell-1,0:fruit : 17  {})})
         (fruit + eqn-based + <ecoservice> : 696015.3639367699 
          {(gridcell-0,2:fruit : 15  {})})
         (fruit + eqn-based + <ecoservice> : 462154.55908067356 
          {(gridcell-0,1:fruit : 13  {})})
         (fruit + eqn-based + <ecoservice> : 515195.3924511202 
          {(gridcell-0,0:fruit : 11  {})})
         }
     )
     (seeds : 4950004.701824954 
      {(seeds + eqn-based + <ecoservice> : 825000.4339876496 
          {(gridcell-1,2:seeds : 20  {})})
         (seeds + eqn-based + <ecoservice> : 825000.6790376918 
          {(gridcell-1,1:seeds : 18  {})})
         (seeds + eqn-based + <ecoservice> : 824999.6759855568 
          {(gridcell-1,0:seeds : 16  {})})
         (seeds + eqn-based + <ecoservice> : 825001.7606296955 
          {(gridcell-0,2:seeds : 14  {})})
         (seeds + eqn-based + <ecoservice> : 825000.96499662 
          {(gridcell-0,1:seeds : 12  {})})
         (seeds + eqn-based + <ecoservice> : 825001.1871877399 
          {(gridcell-0,0:seeds : 10  {})}
         )}
     )
     (B.exemplarii : 6 
      {(super-individual + B.exemplarii + <plant-array> : 3 
          {(B.exemplarii_1 : 6  {})})
         (individual + B.exemplarii + <example-plant> : 1 
          {(B.exemplarii_4 : 9  {})})
         (individual + B.exemplarii + <example-plant> : 1 
          {(B.exemplarii_3 : 8  {})})
         (individual + B.exemplarii + <example-plant> : 1 
          {(B.exemplarii_2 : 7  {})})
         }
     )}
 )
\end{verbatim}

Analogous trees can be constructed which can be used in a similar way
to match spatial patterns. We do this by identifying the neighbours of
a region with symbolic variables in the labels of nodes; a very simple
example using gridcells might look like so:

\begin{verbatim}\label{tree2}
(taxon : 0 {
     (gridcell-1,1 : 1044
      {(gridcell-2,1 : 37 {})}
      {(gridcell-2,2 : 637 {})}
      {(gridcell-1,2 : 937 {})}
     )

     (gridcell-2,1 : 37
      {(gridcell-1,1 : 1044 {})}
      {(gridcell-2,2 : 637 {})}
      {(gridcell-1,2 : 937 {})}
     )

     (gridcell-1,2 : 1044
      {(gridcell-2,1 : 37 {})}
      {(gridcell-2,2 : 637 {})}
      {(gridcell-1,1 : 1044 {})}
     )

     (gridcell-2,2 : 637
      {(gridcell-2,1 : 37 {})}
      {(gridcell-1,1 : 1044 {})}
      {(gridcell-1,2 : 937 {})}
     )

     }
 )
\end{verbatim}

Like the status trees in the first example, these trees can be
compared to reference patterns to identify spatial distributions which
require the intervention of a monitor.  An example of this type of
situation might occur in a model of suburban housing: if large numbers
of residents are leaving in one particular region, there may effects
on the surrounding regions -- increase in civil disorder, for example
-- which are not accounted for by the representations for those areas.

Monitors can be constructed to maintain a list of status trees which
track changes in the ensemble through time. At the moment, this would
be a computationally expensive process, but as the
implementation of the underlying mathematical machinery improves, the
notion of using this kind of higher order analysis of the dynamics in
the system becomes more tenable.


%% \subsection{Partial ordering of trees and nodes}\label{partial-order}
%% We can construct a partial order on nodes relatively simply.  In the
%% first instance we normalise the polynomial labels if necessary (by
%% collecting like terms) and sort nodes using their labels, where the polynomial labels are
%% ordered canonically with indeterminate factors in each term sorted by
%% lexicographic order, and the terms then sorted by degree.\footnote{Of course
%% other sorting strategies are possible.} If two nodes, possess the same
%% label, they are then ordered by their weights.  Should the weights
%% \emph{also} match, they are sorted on the relative order of their set
%% of children, with empty sets taking precedence over sets with
%% children. We may have to exercise some care in ensuring that the
%% lexical ordering doesn't unduly influence our ranking of different
%% configurations. 

%% The sorting order makes the construction of regular, readable output
%% straightforward, since it largely conforms to one of the common
%% patterns used in mathematics and computer science. 


\subsection{Future application of \mclass{introspection} agents}\label{distributedmodels}

The top-level \mclass{introspection} class may ultimately form the
basis for connecting distributed models, a promising library for this
purpose is \swname{$\varnothing$MQ} (``\swname{zeroMQ}''). While,
properly, this lies outside the scope of the work, the approach taken
to address these issues has been structured to make this type of
connection feasible.


\section{Discussion}

The code described is under continuing development. The submodels
which comprise it are deliberately simple in order to maximise the
ability to track the dynamics arising from model switching.  The
models are comprehensive enough that comparisons between ``unimodal''
runs can be made and the parameterisations for the different
representations can be made as compatible as possible.  The model
in \Ctwo was, in many ways, an ideal platform for initial
exploration \ldots except that exploring much further than its ambit
proved unfruitful.  The essential limitation posed by the lack of a
bidirectional interaction suggested that a model capable of
predator-prey interactions was needed.

Early implementation decisions were based on experience with the
models in \cite{lyne1994pmez5,gray2006nws} and \cite{grayningaloo},
along with the general notions formulated in the development
of \Ctwo and \Cthree. The class structure is readily extended, and the
ability to use temporary instances of other agents as intermediaries
-- such as may be found in \mclass{plant-array}:\method{log-data}, where
a temporary plant is used to sequentially hold the essential data to
pass to the tree plotting routine -- makes the process of constructing
new representations much easier than expected.

Current efforts are focused on making the framework more robust and
simpler to use, particularly with respect to mainenance
closures. While constructing closures to maintain variables is
straight-forward in \Scheme, a means of implementation that did not
rely on a reasonable fluency in the language would be better.
Similarly, additional support routines for crafting \mclass{monitor}
agents would help. Reference trees must currently be constructed by
hand, and this can be a somewhat fraught process.


\subsection{Conveniences}
A number of other features of the model have proved to be extremely
useful during the construction of the example of \Cthree, and in playing with
it. Particularly noteworthy items are the parameterisation mechanism,
output generation, the instrumentation to collect the elapsed time
within routines, and the code for debugging output.

Parameter files are small snippets of \Scheme code which adhere to a
particular form. In these files, data are associated with symbols
which get used to determine what state variables are initialised to.
The mechanism which support delayed evaluation of the value to
initialise a state variable allows the modeller to perturb values
(such as mass) in the instantiation of the agents.  Similarly, we are
able to move functions that are traditionally coded as \emph{part of
  the model} into the parameter space -- we could easily write one
basic Galapagos finch model and parameterise the many different
behaviours and food preferences.

\vspace{3mm}
All of the output in the model arises from one of two basic
mechanisms: debugging/warning messages emitted by the ``kdebug''
routines which were briefly discussed in Section~\ref{kdebug} or
output generated by a \mlogger.  There have been a number of benefits
from not incorporating the output within the models.  The most
significant benefit is that it makes the task of producing ``exotic''
output, such as postscript or animations, much more straightforward,
and the code to produce the output is much less likely to interfere
with the model itself, since the output routines are quite distinct
from the models they may poll.  All an output routine needs to know is
how to obtain data from an agent, and how to generate appropriate
output. In this way, the \method{log-data} routines are able to be
applied to a number of quite different models, and their output is
consistent irrespective of the underlying model generating the data.

The debugging messages provided by the \method{kdebug} call and its
kin can be constructed to emit messages according to arbitrary flags
which can consist of symbols, can contain wildcards, class-names, or
parameter taxa.  The same mechanism can conditionally execute more
complex debugging or validation code, and virtually vanishes when the
no messages are required.  This has proved very useful both in
debugging code, and in the parameterisation of models.
{\textbf{Need several ps pages of output (glyphs+tracks, and example txt output)}}

\vspace{3mm}
A small amount of effort has been devoted to improving the efficiency
of the modelling framework -- its function is not so much the
implementation of production models, but as a tool for exploration.
That said, its early versions ran slowly enough, that instrumentation
to do procedure-level and block level timing was added with the
unintended benefit that it fits neatly in the assessment of execution
speed for the purposes of adjusting configurations.  When debugging
unreasonably slow code where it is suspected that model execution is
dominated by a few parts computationally intensive code,  the
\filename{framework} file provides the \Syntax{define\%},
\Syntax{model-method\%} and \Syntax{model-body\%}
alternatives to \Syntax{define}, \Syntax{model-method} and
\Syntax{model-body}. These alternative syntactic keywords add
instrumentation to the code which allows the modeller to determine how
much time was actually spent in the functions, methods or
model-bodies, with a call to \texttt{(timing-report)}'. This make the
task of finding the bottlenecks in the code much simpler.  Access to
the data for assessment can be accessed by calling the
function \method{elapsed-time} with the symbol specified as the tag in
a \method{timing-block}, and these functions are defined in the
file \fname{timer.scm}. 


\section{Cross-representation interactions\label{predation5}}

A model which couples elements which are discrete entities (\eg
cattle) with other that have some type of continuous representation
(\eg a vegetated area) must deal carefully with the way the
interactions are played out. Although interactions that arise are not
limited to predation, predation will be used as an example that
illustrates a way of playing out an interaction between different
representations. 

Models at the ``individual-based'' end of the spectrum can be very
sensitive to fine changes in parameterisations or gradients within the
model.  In some contexts, this is exactly what we wish for, but in
others it can make tuning the model difficult and there may be
underlying structural factors that have undue influence in the model's
trajectory.\footnote{In an unpublished model I wrote in the very early
1990's, simulated fish lined up neatly on boundaries between the
pixels of the digital elevation map that made up the seabed -- they
maximised their access to prey by straddling the different domains.}
Also, processes like predation are computationally expensive when
large numbers of prey are present.  It is possible for us to model
predation between individual-based and population-based
representations, but to do so requires an approach quite unlike the
approaches of conventional individual-based models, where the pursuit,
capture and consumption may be explicitly simulated, or the approaches
of conventional age-class structured population models.  The work
outlined in this section provides a reasonably efficient example of
how coupling and analytic component may be accomplished.

The time taken to simulate interactions between individuals in an
individual-based model is often a significant proportion of the total
run-time, particularly when there are explicit searches for partners
to interact with.  While individual-based interactions are typically
implemented or resolved by simulating (at least to some degree) the
real-world events which occur in the system being modelled, the
interactions between the components of populations are necessarily
less well resolved -- the focus becomes the aggregate effects of the
interaction, and these are often modelled either using integral
transforms (in the case of functional representations) or repeated
evaluations over the histograms representing the populations.

Population models suffer less from this particular
source of overhead since, at least in ecological modelling,
populations are often represented as arrays or function whose elements
or values are an aggregation of individuals with respect to particular
characteristics. In domains where there are a large number of prey,
this is likely to be both more computationally efficient and more
likely to reflect the dynamics in the system being modelled.  It is
easy to imagine conditions where we would like to model either
predator or prey as an individual, and the other as a population.
Relevant examples may be models of relic or marginal populations at
risk of local extinction, or populations where the life-history of
individuals has a bearing on the sustainability of the population as a
whole (such as female elephants).  

%We can map population histograms onto piecewise linear functions with
%the property that the partial integrals of the the linear functions
%correspond to the partial sums of histograms at each boundary
%in the histogram.  If populations are functions, then we can evaluate
%the consequences on the population.

Let us consider the following ``known'' attributes in a system
\begin{table}{ht}
  \begin{center}
  \caption{Symbols\label{symbls}}
    \begin{tabular}{ll}
      $m_a (l)$ & the member distribution with respect to size of each agent of interest\cr
      $G_{i,j} (l, w)$ & the gape filter for predator $i$ with respect to prey \(j\)\cr
      $T (a)$ & returns the taxon or type number of agent $a$\cr
      $M_a$ & the total number of members for agent $a$\cr
      $M^{\ast}_i (w)$ & the sum of all the distributions of agents with a type $i$\cr
      $\bar{M}_i = \int_0^{\infty} M^{\ast}_i (w) d w$ & the total number of entities of type $i$\cr
    \end{tabular}
  \end{center}
\end{table}

\subsection{Calculating mortality}

Let
\[ I_{i,j} (l, w) = M^{\ast}_i (l) G_{i,j} (l, w) \]
and
\[ J_{i,j} (l, w) = M^{\ast}_j (w) G_{i,j} (l, w) . \]
$I_{i,j}$ is the raw distribution of pressure of predator $i$ onto prey $j$,
and $J_{i,j}$ is the raw distribution of the vulnerability of prey $j$ to the
predator $i$.

If the constants
\[ k_{i,j} = \int_0^{\infty} \int_0^{\infty} I_{i,j} (l, w) dl dw \]
and
\[ h_{i,j} = \int_0^{\infty} \int_0^{\infty} J_{i,j} (l, w) dl dw \]
are non-zero, they can be used to scale $I_{i,j}$ and $J_{i,j}$ so that they
form kernel functions, and we get
\[ K_{i,j} (w) = \frac{1}{k_{i,j}} \int_0^{\infty} I_{i,j} (l, w) dl \]
which is the normalised predatory pressure with respect to size, and the
normalised vulnerability
\[ H_{i,j} (w) = \frac{1}{h_{i,j}} \int_0^{\infty} J_{i,j} (l, w) dl . \]
Values of zero in $k_{i,j}$ and $h_{i,j}$ indicate that no predation is
possible -- usually because $M^{\ast}$ has collapsed. In this case we take
either (or both) $K_{i,j} (w)$ and $H_{i,j} (w)$ to be zero.

We calculate
\[ v_{i,j} = \int_0^{\infty} K_{i,j} (w) H_{i,j} (w) dw \]
which has a value in the range $[0, 1]$. If $v_{i,j}$ is non-zero we can
construct the normalised interaction
\[ V_{i,j} (w) = \frac{1}{v_{i,j}} K_{i,j} (w) H_{i,j} (w) \]
which indicates the proportion by size of type $j$ subjected to predation from
type $i$ at the given length $w$. Again a zero value for $v_{i,j}$ indicates
that no interaction (``diner'' or ``dinner'') are possible.



The function
\[ e_{i,j} (w) = M^{\ast}_j (w) V_{i,j} (w)  \]
can be used to give us the the number
\[ E_{i,j} = \int_0^{\infty} M^{\ast}_j (w) V_{i,j} (w) d w \]
which is the exposure of prey population $j$ to the predators in
population $i$. The converse,
\[ C_{i,j} = \int_0^{\infty} M_i^{\ast} (w) V_{i,j} (w) d w, \]
is the potential for predation of the predator type $i$ on an ``average'' prey
of type $j$.

We can then use a predation relationship of some sort to get the raw
number of ``kills'' based on the exposure averaged over the potential
volume (or area) of contact per unit of time, which we call $\Omega_{i,j}$, where $\Omega_{i,j} = \bar{M}_i \mathfrak{F} (E_{i,j} / A_j,
p_{i,j}) \Delta t$ where $\mathfrak{F}$ is the predation relationship,
and $p_{i,j}$ is the parameterisation for the species, and $\Delta t$
is the time step, and $A_j$ is the area/volume we divide by to get a
density.

We can sum over a predator type
\[ \Omega_j^{\ast} = \sum_i \Omega_{i,j} \]
to give us the total possible consumption of prey type $j$.

Alternatively, we can calculate a consumption-by-size distribution and define
the function $\omega_{i,j} (w)$, the raw number of kills for a length $w$ on a
consumption-by-size basis, by
\[ \omega_{i,j} (w) = \bar{M}_i \mathfrak{F} (e_{i,j} (w) / A_j, p_{i,j})
   \Delta t. \]
Thus the impact on the prey population (at least those of length $w$) is
\[ \omega_j^{\ast} (w) = \sum_i \omega_{i,j} (w) \]
and for the whole population it is
\[ \int_0^{\infty} \omega^{\ast}_j (w) d w \]
(or something like that).

Alternatively, we can express things more in the way that it is
calculated in the Atlantis model (\cite{Fulton2011pcomm}) with
\[ Z_i (w) = \frac{g_i M^{\ast}_i (w)}{g_i / c_i + \sum_j a_{i,j} e_{i,j} (w)}
\]
where $Z_i (w)$ reflects the aggregate clearance rate of a predator of type
$i$ if we take $c_i$ to be the ``clearance rate'' which incorporates the
volume it sweeps and a proportion of prey captured and we take $g_i$ to be the
predator's growth rate.

So $\int_0^{\infty} Z_i (w) e_{i,j} (w) \Delta t d w$ is the amount of prey of
type $j$ consumed by the predators of type $i$ over the interval $\Delta t$.



It should be pointed out that the number of types is fairly small compared to
the number of agents, and this shouldn't be too onerous a calculation (at
least compared to playing it all out individually).

\section{Apportioning mortality}

Mortality can be calculated either by apportioning it to each agent according
to the proportion of the global population it represents (and within it,
apportioning the mortality to ages in an analogous fashion), or we can
apportion mortality to each age in each agent according to how much of the
population it represents.

For the agent-by-agent update we have
\[ \delta m_a (w) = \Omega^{\ast}_{T (a)}  \frac{m_a (w)}{M^{\ast}_{T (a)}
   (w)} \]
and for the age-by-age update we have
\[ \delta m_a (w) = \omega^{\ast}_{T (a)} (w)  \frac{m_a (w)}{M^{\ast}_{T (a)}
   (w)} . \]
The new distribution
\[ n_a (w) = m_a (w) - \delta m_a (w) . \]
In these steps, places where there are no members of size $w$ should be dealt
with carefully in the division, and at all points $M^{\ast}_{T (a)} (w)
\geqslant m_a (w)$.
