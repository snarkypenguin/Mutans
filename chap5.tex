% chap5.tex (Chapter 5 of the thesis)

%% classes are refered to in textit, syntactic elements are in texttt
%% filenames are in pandora
\message{Chapter 5}

\chapter[AN EXPLICIT IMPLEMENTATION]{An explicit implementation}
\WeAreOn{\cfive}\label{explicitmodel}


Chapter~\ref{modelefficiency} demonstrated that switching models can
provide benefits in fidelity and efficiency, when compared with
non-switching alternative models.  The example developed in
Chapter~\ref{adaptiveselection} describes a model of a small trophic
network to use to explore how a general model might be
constructed. Here, we describe an explicit implementation of a system
based on Chapter~\ref{adaptiveselection}. The role of this
implementation is to provide a concrete starting point for further
exploration, development and critique.  In this chapter, we describe
the system, discuss the reasons certain approaches have been taken,
and address both how the system could be used when implementing new
models and avenues for further research.\footnote{ \textbf{Randall --
Remember}: HERE \textit{parameter files, structure of output mechanism
(loggers) -- simulation/analogue of actual sampling strategies,
postscript output (glyphs, tracks, graphs, etc), scope for
automatically generating animated output }} The model this chapter
refers to is released under the GPLv3 and available
at \url{http://github.com/snarkypenguin/remodel.git}.

\section{Design considerations}
The general design of this example framework follows conventional
patterns for an agent-based model, and it was influenced by my
previous work (\cite{lyne1994pmez5, gray2006nws, gray2014}) designing
and implementing the core of operational models for management
strategy evaluation (MSE) dealing with anthropogenic effects on
marine ecosystems.  The principle behind MSE and adaptive management
is that candidate strategies for managing the effects of human
activity are explored in simulation\footnote{This is effectively
  impossible to do in the real world, since the application of a
  strategy has a high likelihood of changing the baseline for other
  candidates in the trial.} Adaptive strategies actively monitor
conditions in their domain and change management strategies to suit
the observed conditions and dynamics of the
system. (\cite{walters1976,smith1993,polacheck1999,sainsbury2000,keith2011})
Each of these three major projects has provided the opportunity to
explore the issues associated with modelling increasingly complex
systems over increasingly large areas.

The model in \cite{lyne1994pmez5} was a purely individual-based model
with integer time steps, though the time steps varied with the species
being simulated. Like the example model, all activity was coordinated
by a queue of entities that were sorted on when they were next due to
run. This model worked in a floating point space and the simulated
organisms exhibited modal behaviour: searching, pursuit of prey,
eating and movement in a possibly drunkard's walk with migratory
weighting applied in the case of some species.

While \cite{gray2006nws} used many of the approaches from its
predecessor, its focus was considerably more broad, and required a
much more flexible approach to the problem of simulating the
interactions of human activity and components of local ecosystems. In
this model time was treated as a continuous variable, and a number of
new kinds of simulated entities were required to reflect the
activities influencing the system and the management of these
activities. This broader canvas required a more abstracted view of
what constituded an ``agent'' in the system, and extended
\cite{lyne1994pmez5} in the way it dealt with the simulated
populations, treating some as collections of instances of individual-
or super-individual-based representations and others as aggregate
representations.  The model also used distinct representations for
different life-stages of some species within the model.  Care was
taken so that entities within the model as a whole could interact
consistently regardless of their particular representation.

\cite{gray2014} was like \cite{gray2006nws} in that it addressed the
problem of the regional management of human activity as it impinged
upon the coastal ecosystems, but it incorporated a much more diverse
set of components, including terrestrial land-use, local economies,
marine tourism and recreation, and coastal industries such as salt
production and fishing. The major difference between this model and
\cite{gray2006nws} was that in this model, the simulated whalesharks were
removed from the common domain as a part of their annual migration and
reproduction cycle. 

In this context, the aim of example model was to incorporate a way of
using the states of the participating agents to decide on an
appropriate mix of representations for the simulated entities, and the
mechanisms that support changes in the representation of these
entities. The maintenance of state for superceded representations
evolved from the basic strategy developed in
Chapter~\ref{modelefficiency}.  Since the only interactions in
Chapter~\ref{modelefficiency} were between the simulated organisms and
the environmental plume, there was no need to include mechanisms for
agents to interact in a more general sense.

\section{Model implementation}

The model is based on the the description of the example in Chapter
\ref{adaptiveselection}. In the example, the domain consists of nine
cells containing tree-like plants. These trees are a critical food
source for a population of herbivores, and the trees rely on the young
herbivores to eat the ripe fruit in order to make their seeds viable
(perhaps the seeds have a particularly tough seedcoat). A carnivore
which preys solely on juvenile herbivores is introduced into a stable
system.  In Chapter~\ref{adaptiveselection}, the introduction of the
carnivore and a subsequent rangeland fire initiated a sequence of
events which, in turn, engendered changes in the configuration of the
model as a whole.  The implementation described here is not identical
to the system described in the paper -- some inadequacies of the model
presented in the paper have been addressed (such as the inability of
the plants to recolonise a cell after they have become extinct in it).


\subsection{Scheme}
The design decisions for the modelling framework was heavily
influenced by my experience with large ecosystem models which
primarily used \Cpp and \CC.  While these models served their purposes
well and I incorporated a number of what turned out to be
``scheme-flavoured'' approaches in the handling of data, the
development of non-traditional components was made
difficult.\footnote{This was particularly well exemplified by the
problems associated with modelled entities (whales) which would leave
the domain of the model for much of the year, and during this period,
they would calve.}

The example model was implemented in the \Scheme programming language.
\Scheme was chosen because it has long possessed properties that are
only now becoming part of the standard in members of the \CC family,
such as anonymous functions, and first-class closures. Closures are a
natural way the data we want to preserve across transitions between
representations. The data is encapsulated in a maintenance
closure which holds and updates the data from a
representation which has been replaced. \Scheme was designed with a
minimalist approach and for exploring fundamental ideas about language
design, such as \texttt{call-with-current-continuation} (often
shortened to \texttt{call/cc}).  Scheme is a block-structured,
lexically scoped language, and while the preponderance of parentheses
(as well as brackets and braces in some dialects) can be daunting to
newcomers, its small size and simple syntax make it relatively quick
to learn.

I have specifically avoided the use of \texttt{call/cc}, arguably the
language's \emph{enfant terrible}. It is a control construct which is
an obvious avenue for implementing the multitasking system in the
model, but, somewhat like the bait in the trap, there are a number of
costs that make its use in a project of this nature unappealing: most
importantly, the behaviour of \texttt{call/cc} is not intuitively
obvious to most modellers and programmers, and it brings additional
complexity in debugging, and memory overheads with their associated
run-time costs.

A number of factors were important in the choice of implementation
language, namely
\begin{itemize}
\item[--] class-based multiple inheritance with polymorphism\\
\item[--] weak/dynamic typing\\
\item[--] linking with compiled code in other languages,\\
\item[--] able to support maintenace models\\
\item[--] potential for parallelising and distributed execution.
\listintertext*{and}
\item[--] interpreted operation if possible,
\end{itemize}

I considered \Cpp and \CC as implementation languages, since they
both\footnote{It isn't technically hard to construct a class system
for \CC, with the desired properties, but there is a reasonable amount
of effort, and the result would be idiosycratic.} can address most of
these factors. I was disinclined to use other obvious candidates,
notably \Java, and \Csharp, and \DD do not support multiple
inheritance, and are strongly typed.

Using an interpreted language also meant that I could readily
encorporate both numeric quantities with physical units and functions
in the parameterisation of modelled entities in the parameter files
(\emph{c.f.}  the files for the
\method{mass-at-age} functions for animals and plants). \Scheme was,
at its inception, designed to be a tool for exploration
\cite{sussman1998first} and it remains an excellent tool for this
purpose. \Scheme is an actively developing language, and there are a
number of very good interpreters and compilers for it; many of these
implementations are also amenable to integration with other languages,
either through directly linking binary objects or by interacting
through a virtual machine (such as a JVM).


A simple example of a closure might look like so:
\begin{verbatim}
;; The closure will be called like (@ 'tick dt) 
(define @
  (let ((TBT 0) ;; tributyltin
        (depuration-rate 0.0001) 
      (lambda (action #!rest x)
       (cond
        ((eqv? action 'reset) (set! x 0))
        ((eqv? action 'value) TBT)
        ((and (eq? action 'tick) (= (length x) 1) 
           (number? (car x)))
         (set! TBT 
           (* TBT (- 1 (exp (* depuration-rate (cadr x)))))))
       (else 'ignored)
      ))))

\end{verbatim}
This example maintains a contaminant level in the absence of contact
with the contaminant, TBT.  Should the representation of the
modelled entity enter a situation where TBT is present, a monitor
class may decide that a representation change is necessary, and the
closure would be queried for the correct value for the contaminant
level to use in the construction of the new representation.

The variable \sttvar{TBT} is set to point to the function the
\Syntax{lambda} defines, and the state variable \sttvar{counter} is
``global'' to this function, rather like a static variable defined
outside a function in a C source file. Closures are a
coupling of a procedural element with data which is preserved across
invocations of the procedure.

\subsubsection{Interpreter/Compiler}
I have used
\GambitC,\footnote{\URL{http://gambitscheme.org}} a well
regarded implementation of \Scheme which runs on all of the major
platforms (\Linux, \Unix, \OSX, \Android and \Windows). \Gambit
incorporates both a mature \Scheme interpreter and a compiler, and
both can dynamically link programs with external, compiled code and
libraries which may exist either as compiled \CC or \Scheme code.  The
choice to use \Gambit, rather than another version of \Scheme,
was heavily influenced by several points: it was the implementation
used for the model in Chapter~\ref{modelefficiency}; its ability to
link to compiled \CC and its own compiled code can combine to produce
programs which are noticably faster than purely interpreted code; it
supports very \CC-like infix notation (indeed, it can be programmed
using syntax which is almost-native \CC syntax); and adding
hand-crafted \CC or \Cpp code for particularly intensive routines is
not difficult.

Porting the model to other versions of \Scheme should be relatively
simple; the only potentially awkward issue in using a
different \Scheme implementation would be the use of \sdefmac in the
creation of the classes and methods in a model.  These macros are used
to automatically incorporate code which maintains data-structures that
provide information essential to the communication between submodels,
and the system as a whole. The macros are also able to detect and
respond to some sorts of oversights or inconsistencies in the
construction of methods or model-bodies.  In principle, \sdefsyntax or
other tools for syntactic extension or modification would be better
means of obtaining many of the effects of the macros defined in the
\fname{framework} file: \sdefmac was the basis for the first
incarnation of this framework, and -- at the time -- was the more
familiar mechanism, and I made the decision to perservere with it.

The use of \sdefmac complicates the process of locating syntax errors,
since the interpreter is unable to track the line numbers within a
macro. \footnote{The macros which wrap the methods called by an agent
are implemented in such a way that the code they produce can be
written to a file, and that file may be included in the place of the
original block of code.  In this way, the shortcomings with respect to
line number tracking can be ameliorated somewhat.  Such code could be
used to generate a code-base for a model which was free of macros.}
The only aspect associated with using macros which would require
broader changes with \sdefsyntax is synthesising the context-specific
identifiers (such as
\symb{<animal>-model-body}) which refer to the ``body'' of the
different classes of agents; this is not insurmountable, but will be
left for a subsequent version of the modelling framework.

\subsection{\SCLOS -- a \Scheme implementation of \CLOS}

The class structure in the example model makes use of the \Scheme
implementation of \SCLOS that was written
by Gregor Kiczales \cite{kiczales1993xerox} while he was working at
Xerox PARC in 1992 and 1993.  Kiczales has been an instrumental
researcher and proponent for the use of metaobject protocols (MOPs) as
a tool for making computer programs clearer, more efficient, and more
robust.  The basic tenet is that generic methods or functions are used
to manipulate objects, and that these generic objects inspect the
nature of the data being passed to them and pass the processing to a
specialised function which deals with the task most appropriately.
While this is not an exact analogue to the problems we seek to address
in this work, there is a substantial similarity and using \SCLOS
with its implicit MOP seemed a natural fit.  \SCLOS has been
the basis for many of the significant object-oriented  systems for
\Scheme.  In the development of the model there have been some
instances where the ``wrong'' methods seemed to have been
called to act on an object, but these have inevitably been associated
with a type error in the signature of an overloaded method (in
the \SCLOS idiom, that would be a generic method with more than one
implementation), or a case where a generic method has been defined
twice. The modelling framework now protects against duplicate generic
methods. 

The basic \SCLOS library has been slightly modified
(the \mclass{<object>} class of \SCLOS has been renamed
to \mclass{primitive-object}) and additional support routines have
been added in the file \fname{sclos+extn.scm}.  These additions
include the recognition of a number of \Scheme data-types, routines to
examine the parents of an object, registers which recognise objects
and classes which have been instantiated, extended support for the
initialisation of slots in an \SCLOS object (parameter files), and
classification predicates.  

\subsection{Class structure}
The submodels in the example are all derived from a basic
\mclass{agent} class which provides each modelled entity the
facilities for interacting with the kernel of the model and mechanisms
for the agents to obtain information about the other agents and a
means of interaction.

The taxonomy of the classes is fairly conventional: the primitive
\magent is the superclass of almost all of the model components
(exceptions include classes for things like bounding regions), and it
provides submodels with access to the infrastructure of the system. 

At the lowest levels, the classes provide definative features that
characterise broad categories of entities, such as discrete locations,
delinated areas, the ability to map between ordinate systems, or the
ability to poll other agents at regular intervals.

\begin{table}[H]
\begin{center}
  \caption{Fundamental classes in the model -- \fname{framework-classes.scm}\label{classtable1}}
  \begin{tabular}{l L{7cm}}
    \toprule 
    \textbf{Classname} & \textbf{role} \\
    \midrule
    \mclass{agent} & The agent class contains all the fundamental
    components for a model to run within the framework, such as
    essential state variables and common methods \\
    \mclass{projection} & This class is derived from \mclass{object};
    it adds methods and state variables to allow mapping from an
    agent's native coordinate system to the common coordinate system
    and back. This is used extensively by the routines which output
    graphical data -- mapping each agent from its domain to the domain
    of the output page or image.\\
    \mclass{introspection-agent} & The methods which allow
    \mclass{monitor} and \mclass{log-introspection} agents to operate is provided by
    this class -- it forms the basis for agents which poll other agents \\
    \mclass{plottable} & Children of this class can be plotted in
    map-like output. The attributes of this class include a ``glyph''
    a location, an orientation, a preferred font, colour, glyph size
    (which may be scaled by the value of a slot) and an additional
    magnification which can be used to adust the scale.
    This is the class that provides location and orientation to the
    animal and plant classes.\\
    \mclass{environment} & The basic spatial context is provided by
    this class; it maintains a bounding
    box and a link to an explicit representation\\
    \bottomrule
  \end{tabular}
\end{center}
\end{table}
The classes in Table \ref{\classtable1} 



\begin{table}[H]
\begin{center}
  \caption{More fundamental classes -- \fname{framework-classes.scm}\label{classtable2}}
  \begin{tabular}{l L{7cm}}
    \toprule 
    \textbf{Classname} & \textbf{role} \\
    \midrule
    \mclass{object} & This is the ``primal'' class for all of the
      different agent classes, and the more complex data structures
        like polygons  \\
    \mclass{array} & Arrays are used to construct a collection of
    lists which often act as the basis for a model representation
    between a fully defined individual-based representation and an
    analytic representation.  Lacks the potential temporal sensitivity
    of an individual-based representation, but able to maintain many
    i-state variables (\textit{sensu}\/\cite{caswell1992individual})\\
    \mclass{diffeq-system} & implements a class which runs a
    Runge-Kutta4 algorithm for a system of differential equations.  This
    class doesn't maintain variables as such, rather it is passed
    ``getters'' and ``setters'' which obtain state variable values from
    other agents (such as \mservice agents) and subsequently
    set new values in those agents.\\
    \mclass{proxy} & Proxies represent some ``element'' (like a row in an
    \mclass{array} agent) when engaging in individual-to-individual
    interactions.\\
    \mclass{model-maintenance} & provides the support for maintaining
    state variables for representations which are not currently active
    (such as in Chapter~\ref{modelefficiency}).\\
    \bottomrule
  \end{tabular}
\end{center}
\end{table}


\subsubsection{Introspection agents}
A special class of agents, the \mclass{introspection} forks into two
branches derived from \mclass{logger} and \mclass{monitor} exist
solely to poll the other agents within the system and either extract
information in an analogue of the sampling undertaken by researchers,
or to monitor the state of agents, ensembles and the model as a whole
and effect change where necessary. The loggers handle all of the
generation of output files -- the code which makes up the submodels
only provides one or more \method{log-data} methods which the loggers
use to assemble their output.  The notion is that the agents do not
need to know about what is being sampled, only how to make the data
they can provide ``presentable'' and the loggers don't really need to
know the details of a submodel's workings, only that it ought to
obtain certain data from agents of particular types and generate an
output file.

While this approach to data output engenders some extra overhead, it
has a number of advantages which are not necessarily
obvious. Consolidating the output into separate agents makes changes
to the format of the generated output a simple matter of changing
which output agent is used, or which format is required, and the same
mechanism, or one which is very similar, could be used as the basis
for coupling distributed models.  Using \mlogger agents rather than
embedded output also means that we can confine code associated with
``output'' code (mainly the \method{log-data} methods) to a very small
section of a few basic parent classes; thus instances of all the child
classes can automatically be queried without the necessity of writing
new output routines.  The MOP of \SCLOS is used to map calls from the
introspection agent to the appropriate version of \method{log-data},
and the chain of inheritance can be used to call the \method{log-data}
methods of its parents to structure the output appropriately.

\begin{table}[H]
\begin{center}
  \caption{Introspection classes in the model -- \fname{introspection-classes.scm}\label{classtable2}}
  \begin{tabular}{l L{7cm}}
    \toprule
    \textbf{Classname} & \textbf{role} \\
    \midrule
    \mclass{log-introspection} & The fundamental mechanics that
      underpin the regular polling of other agents in the system\\
    \mclass{logger} & {provides the basic structure for generating model output}\\
    \mclass{monitor} & {provides the machinery for assessing the state
      of the system and effecting changes in its configuration}\\
    \bottomrule
  \end{tabular}
\end{center}
\end{table}
The introspection classes are defined in the files
\fname{introspection-classes.scm}, \fname{monitor-classes.scm}, and
\fname{log-classes.scm}.  Of these, \fname{log-classes.scm} is the
most complex.

The monitor classes are able to maintain a history of the decisions
they make and can, in principle, factor this history into the
decisions they make subsequently.  As yet, the \mmonitor classes are
still quite simple, acting to coerce changes in the constitution of
the model ensemble according to fairly simple rules.




The individual-based classes are all derived from a small ensemble of
classes which endow them with the physical attributes we expect, such
as location, and mass. All of the biotic agents in this branch of
classes are able to maintain a history of their movement through the
model's geographic domain and may also maintain a record of state
variables. The agents may inspect these data and potentially use them
in their own decision-making process; thus, some (as yet
unimplemented) model of a animal may ``recall'' a recent encounter
with a predator in a location and avoid the area.

\begin{table}[H]
\begin{center}
  \caption{Basic classes for individual-based rep\-re\-senta\-tions
  -- \fname{framework-classes.scm}\label{classtable3}} 
  \begin{tabular}{l L{7cm}}
    \toprule 
    \textbf{Classname} & \textbf{role} \\
    \midrule
    \mclass{tracked-agent} & Agents derived from this class automatically
    maintain a time-stamped history of where they have been. This data
    can be extracted for analysis or plotting.\\
    \mclass{thing} & adds mass \\
    \mclass{living-thing} & This class adds age and mortality, also aware of the
    environmental context in which it lives \\
    \bottomrule
\end{tabular}
\end{center}
\end{table}

The classes for the simple organisms of
Chapter~\ref{adaptiveselection} are defined in the files
\fname{animal-classes.scm}, \fname{plant-classes.scm},
\fname{landscape-classes.scm}, with corresponding
``\fname{-method.scm}'' files with the implementation of the methods.

\begin{table}[H]
\begin{center}
\caption{Environments -- \fname{landscape-classes.scm}\label{classtable4}}
\begin{tabular}{l L{7cm}}
\toprule
\textbf{Classname} & \textbf{role} \\
\midrule
\mclass{ecoservice} & Usually associated with a patch, this class
provides a data value which can be inspected and modified by other
agents.  It is possible for the ecoservice to access data in other
agents, and to have implicit growth or decay associated with its
value.  Instances are able to store the (time value) pairs in a
history list.\\
\mclass{population-system} & This is a refined version of
the \mclass{ecoservice} which primarily provides a simple ``biomass''
agent.\\
\mclass{patch} & The basic geographic region is implemented in
\mclass{patch}. It has a bounded area,  a list of ecoservices which
are assumed to be uniformly available, a ``notepad'' which can be
interrogated for state information, and it may also have ``caretaker''
routines defined in parameter files which get executed every time the
patch gets a time step.\\
\mclass{dynamic-patch} & Patches  in this class may have a system of
differential equations which stand in the place of the simpler --
possibly simplistic -- dynamics possible using only the ecoservice
mechanisms.\\
\mclass{landscape} & An instance of this class maintains a list of
patches and a terrain function which provides an altitude ordinate to
the locations in the geographic domain; in aquatic/marine models, this
would typically be used to provide the depth of water at a give point.\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

%%%% plant classes

Plants and animals are both implemented in classes derived
from \mclass{living-thing}. An analytic representation for plants
based on \mclass{diffeq-system} is also implemented.

There are essentially three implementations for modelling plants in
the example model. \mclass<plant> is the basic starting point for
the \mclass<example-plant>, which implements a model substanially
similar to that described in Section \ref{adaptiveselection}.\ref{plantch3}. The alternative
representation for plants is the \mclass{plant-array} which is an
array of state-vectors which apply update methods to each of the state
variables which are similar to those used for the individual based
version. \mclass{plant-proxy} is an interlocutor for
a \mclass{plant-array}, though we have not employed it in this model.\\
%%%% animal classes

The treatment in the animal classes is similar to that of the plant
classes. Both the \mclass{aherb} and \mclass{jherb} can be represented
by \mclass{animal-array} agents, but we refrain from carrying the
generalisation to include the \mclass{carnivore} class -- carnivores
are unlikely to reach high population levels, and their behaviour is
more explicitly discrete.

\subsection{Parameterisation}

Model agents (instances of the above classes) are made by calling the
\method{create} function.  The principal tasks of \method{create} are
to allocate storage, and to call the functions which initialise the
state variables of an agent.  \method{create} initialises its state 
variables using the data from the files in a nominated parameter
directory. The state variable initialisations specified in
class--based files are applied first from the parent class farthest
away in inheritance, through to file associated with class of the
agent being created. It is quite likely that a given state variable
may be reset several times in the process as the model representation
is refined. Finally, the state variable settings from a file
associated with a \emph{particular}\/ set of instantiations -- the
agent's taxon -- are applied.  A model might deal with several
populations of distinct species (taxa) of tuna.  These taxa may have
different prey preferences, temperature preferences, and metabolic
parameters.  Each distinct species would load its initialisation
parameters from a taxon file associated with the species.

In order for the initialisations indicated in these files to be
applied in the correct order, class--associated files must have the
same name as the class they relate to (such as \fname{<tuna>} and
\mclass{tuna}), and taxon-associated files will have the same name
(including case) as the taxon used (such as
\fname{T.albacares} and \strng{T.albacares}).\footnote{It doesn't matter if there are
  entries for things which are not state variables -- any specification
  which isn't recognised is silently ignored.}

Parameter files contain \Scheme code and they are loaded
immediately after all of the code associated with implementing
representations of submodels has been loaded, but before any model
initialisation occurs. 

The initialisation data for agents is primarily taken from files in a
nominated directory (in our case ``\fname{./parameters/}).  Each file
is associated with either a particular class, or with a particular
taxon.  When agents are created, the instantiation process first
initialises all the state variables to \symb{<undefined>} -- a value
used to identify an uninitialised value.  The initialisation then
applies any initialisations which may be found in the file associated
with the \mclass{agent} class, namely \fname{./parameters/<agent>}.
The process continues, from most general to most specific, till any 
initialisations found in the agent's parent classes have been
applied.  The system then looks for a ``taxon'' file which which
contains very specific initialisation data -- we might have a generic
\mclass{quercus} class, for example, which serves adequately for both the
\fname{Q.robur} and for the \fname{Q.petraea} taxa, but the species
specific differences are assigned last, and take precedence.  Finally any
initialisation indications found in the \method{(create \ldots)} call
used to instantiate the agent are applied.  In this way, more specific
parameters (like metabolic rates) get applied later in the process.

A typical parameter file might look
like look like the '\filename{<example-plant>}' file below:
\label{parameters}\label{kdebug}
\begin{verbatim}
'Parameters
(kdebug '(loading-parameters 
          taxon-parameters "B.ex-longevity" "*.ex-*") 

(define B.ex-longevity (* 37 years))
(define B.ex-mass-max (* 150 kg))
(define B.ex-mass  
   (rk4 (lambda (t y) ;;   This is the diff'l eqn
			(* 5e-7 (- 1 (/ y B.ex-mass-max))))
				 0 B.ex-longevity (* 4 weeks) 0.1))
	;; step size is 4 wks, 0.1 is the initial size 
   '' of the tree when we start tracking
(if (and (kdebug? 'check-mass-at-age)
     (not (test-mass-at-age B.ex-mass)))
   (error "Mass at age failed in B.exemplarii"))

(define <example-plant>-parameters
  (list
    (list 'cell '<uninitialized>)
    (list 'peak-mass 12)
                 ;; this will seed trees in the
                 ;; 4km x 4km square around
                 ;; the origin
    (list 'location 
          '($ (random-location (list -2000 -2000) 
                               (list 2000 2000))))
    (list 'mass '(\$ (nrnd 4 1 0.01 12)))
    (list 'height '(\$ (* kg (min 12 (+ 12 (nrnd 10))))))
    (list 'fruiting-mass (* 8 kg))
    (list 'fruiting-prob (/ 0.1 week))
           ;; 10% per week
    (list 'fruiting-rate (* 7 (/ 1 kg)))
           ;; seven fruit per kg of tree
    (list 'mort-mass '<uninitialized>)
    (list 'mort-prob (/ 0.1 year))
    (list 'seeds-per-fruit '(\$ (+ 4 (urnd 8))))
    (list 'habitat '<uninitialized>)
    (list 'mass-at-age B.ex-mass)
    (list 'references 
      '(J. Muppet Botany, S.Chef et. al. v2,1995))
    (list 'note "This is enough for things to run")
  )
)
(set! global-parameter-alist 
   (cons (cons <example-plant> <example-plant>-parameters) 
         global-parameter-alist))
\end{verbatim}

The \symb{'Parameter} line which is the first entry in the file is
an indicator that this is a parameter file, and \Scheme
programs (like the modelling framework) can readily use this to
determine that the file is not actually code intended to be executed.
The definition which follows is a list of lists where the first
element of each of the second level lists must be a symbol.  These
symbols should (but are not required to) correspond to the state
variables in the agent. Note that even though this file is associated
with \mclass{example-plant}, it is not restricted to setting state
variables defined in \mclass{example-plant} -- it is \emph{both reasonable
  and common} to set variables associated with parent classes, such as
its \sttvar{location}, above, which ultimately comes from the \mclass{<thing>}
class.

The last three lines of the file add the definitions to the list of
all the state variable defaults the system knows about -- if they are
missing or compromised, the definitions may not be accessible.

The parameter files from which the parameterisation is taken are
scheme files, and can contain variable and function definitions.  The
example above includes the function definition for the mass-at-age
function for the taxon \strng{B.exemplarii}.  Na\"ively we might assume
that we would be able to perturb the starting mass of each individual
tree by just incorporating a random number into the mass indicated in
the parameter file: we can do so, but the parameter
files are loaded early in the loading process, before any agents are
actually instantiated.  At the point we begin to create agents,
the parameterisation data is static.  In order to overcome this, the
code which accesses parameters recognises that parameters of the form
\symb{(\$ \ldots )} should be evaluated dynamically when they are
accessed and passed back, so the line

\begin{verbatim}
   (list 'height '(\$ (* kg (min 12 (+ 12 (nrnd 10))))))),
\end{verbatim}

will return a value greater than twelve, with a distribution like the
left half of a normally distributed variable with standard deviation
of ten.  Other examples can be found in the parameter files with
\fname{carnivore}, \fname{adult-herbivore},
\fname{juvenile-herbivore} and , \fname{B.exemplarii}. 
It should be emphasised that the parameters obtained from the
parameter files may be overridden in the \method{create} call, and by
code within the model itself.

In general, units are specified in the parameter
files, and should also be specified when indicating particular
initialisations for agents.  The (extensible) list of known units is
found in the file \fname{units.scm}.


\subsection{Model initialisation}

The first stage in model run is the creation and initialisation of the
components of the model. A model is constructed by a (comparatively)
short \Scheme program which uses the infrastructure to create
and configure the agents which comprise the model ensemble.  These
agents are added to a list called the \variable{runqueue}, which
corresponds to the execution queue in a simple multitasking operating
system. The agents are created by a call to a routine in
\fname{sclos+extn.scm} that looks like
\begin{verbatim}
    (create <landscape> taxon 'name "domain" ...)
\end{verbatim}
where the ellipsis indicates additional state variable
assignments which supercede any previously set values from the
parameter files. Agents which have an associated region or location
would usually require some additional code to to add agents to
encompassing entities (such as including trees in the landscape) this
would often be a part of the code which creates the
agents\footnote{See the file \fname{specific-model.scm}, or spawning
  code in \fname{animal-methods.scm} for an illustration of this.},
or would be effected in the class-method \method{initialise-instance}
which is called as a part of the process that prepares an agent to be
run; generally, initialisation of an agent would be accomplished using
both of these approaches.


\subsection{Methods, model bodies and closures}

Methods are functions which are specifically associated with instances
of a class.  \SCLOS supports having a number of distinct methods with
the same name, but different arguments, as determined by their type.
This means that \mclass{shark}and \mclass{seal} may both have an
\method{eat} method -- moreover, \mclass{seal} may have two
\method{eat} methods: one which recognises when the item being eaten
is an \mclass{animal}, and one where it is merely an \mclass{object},
which would usually be inedible.  This multiple-dispatch means that no
explicit test is required in the model code (beyond writing an
\mclass{object} version which spits out the offending item).  If there is
only an \mclass{fish} version of the \method{eat} method for
\mclass{seal} agents, they will only attempt to eat fish, and
presenting them with anything else will raise an error.  This makes
the extension of classes much less vulnerable to mishandling
interactions with instances of other classes.

Model bodies are special methods which are only called by either the kernel
or by the subsidiary execution lists embedded in agents. There is no
provision for an agent to directly call model bodies.

Model methods are essentially functions, except that they are
explicitly associated with the class signature of the arguments passed
to them. For example, there are several versions of the model method
\method{dump}, and each is made distinct by the class of its first
argument and may be invoked only by members of that class and its
descendants.  This ability to have several functions with the same
name is made possible by the use of instances of
\mclass{generic-methods} which maintain a list of actual functions and
information about the arguments they expect.  The generic function is
able to recognise the types of the arguments it is passed and to
forward the call on to the appropriate actual implementation.  There
are two consequences to this approach which ought to be mentioned
\begin{description}
  \item[Only define each generic method once] Redeclaring a generic-method
    (\Syntax{define} \method{hunt} (\method{generic-method})) hides any previously
      declared \method{hunt} methods (generic or otherwise!) out of
      the model's code path. To this end, all declarations of generic
      methods are either in the \fname{declarations.scm} file, which
      contains the declarations for most of the model-methods or in
      the \fname{framework-declarations.scm} file which declares the
      generic-methods for methods required by the lower levels of the
      framework.  Related to this is the problem of defining a method
      before you have a generic-method declared --  the macros in
      \fname{framework} should catch this.
  \item[Don't construct methods that match the same argument
    lists] Generally, \SCLOS is very good at getting the right
    method for the job, but there are situations where it can be
    confusing for it.  If you have a situation where there are several
    possible desired code paths for essentially similar arguments, it
    is clearer to have an explicit selection made within the body of a
    single method, and much easier to debug.
\end{description}
 
Each agent's \Syntax{model-body} is run by the kernel at each of the
agent's time steps; there is no \Syntax{object-body} since objects are
(by design) not able to be run by the kernel.

\section{Execution and Control flow}

After the initial cohort of agents have been instantiated and
introduced into the \symb{runqueue} (or in the
\sttvar{subsidiary-runqueue}s of other agents), control is passed to
the kernel which then begins to run each agent in turn for an
appropriate time step. The queue is ordered first by the
\sttvar{subjective-times} of the agents, then their relative precedence, and
finally an optional \sttvar{jiggle} value which can be added to ensure that
there are no systematic preferences within a time step.

The most important routines in maintaining an orderly flow of control
from one agent to the next are
\begin{itemize}
\item[\method{run-agent}] which manages the interactions between the
  agent and the \variable{runqueue} and constructs the procedure that the agent uses
  to communicate with the kernel.

\item[\method{run}] calculates the upper limit on the amount of time the
  agent will run, runs the body of the model (declared in a
  \Syntax{model-body} block), runs any nested agents and attends to 
  the maintenance of any data from superceded representations.
\end{itemize}

The sorted queue of agents is maintained by the system; the agent at
the head of this list is removed from the list and execution is passed
from a call to \method{run-agent} to \method{run}  which then passes
control to the agent's \Syntax{model-body}. The call chain is split in
this way to separate interactions with the run queue and the
management of the activities of agents.  The motivation for this
separation is that provides an avenue for changing the nature of the
system used to run the models, say from a single-threaded execution
list to a number of parallel lists.

The \Syntax{model-body} of an agent takes control with an indicated
maximum amount of time over which it might run.  Agents need not run
their whole time step -- events may occur which cause them to truncate
their turn, returning control to the kernel with an indication of the
amount of time they actually used.  When the kernel receives control
from an agent, it examines the data passed to it by the agent and acts
accordingly: terminated agents may be silently dropped from the queue,
for example, and other agents may be reinserted in an appropriate
place in the queue for the their next time step, or agents
representing organisms which have just reproduced may introduce new
agents into the \symb{runqueue}.

Control also passes from agents to the kernel when the agent makes
queries looking for prey or to interact with other agents: a carnivore
might want to know whether there are any prey animals within a certain
radius, for example.  Here, a request would go to the kernel for a
list of nearby prey.  The kernel would then examine the list of agents
meeting the requirements (type, spatial location, temporal contiguity)
and pass the list -- and control -- back to the hunter.  This is a
common paradigm in both agent-based modelling and in
computer-operating systems design, where access to information or
resources are obtained from a delegated ``authority''.

The maintenance of state data from a superceded representation is
rather different. When an agent is given data to maintain, it
processes them at the end of each of its timesteps. Each is asked each
what set of data their maintenance closure needs in the update step;
the set of data is obtained control and the data to run the maintenance
closure. When the closure has finished its update, control returns to
the agent, and control ultimately passes back to the kernel.


\section{Interactions}\label{interactions}

The first step in an interaction between agents is often to find other
agents which fit particular criteria: a predator looks for nearby prey
or a travelling salesman might look for accomodation within their
budget, for example.  Many of these searches are conducted either by
using one of the \method{locate} calls provided by the kernel, or by
taking advantage of previously cached information maintained by the
searcher. The searches can be restricted to particular spatial
regions, particular sets of taxa (as determined by the agent's
\sttvar{taxon}), particular classes, or the possession of particular
state variables (such as \sttvar{location} or
\sttvar{available-rooms}). The nature of the interactions may range from
merely ascertaining the presence of another agent, to predation, or
extracting state information (such as mass, location or the price of a
room).  

\subsection{Spatial queries}

Agents that regularly interact with other agents which are close to
them in the spatial domain will often cache references to their
important neighbours to reduce the overhead of calls to the kernel
routines, since direct communication with target agents has a much
lower overhead. Direct contact does require agents to check to see
whether the agent being queried is active or has been replaced by
another representation, since both state and representation changes
can occur without the agent knowing. Most state changes of an agent
will be from \symbolic{alive} to \symbolic{dead} or \symbolic{terminated}: the first two
indicate that the agent is still available for interaction, the third
indicates that it is no longer a participant in the system. Should the
object of an agents interest become \symbolic{terminated}, the agent can
then query the terminated agent to see if it has been replaced by
another representation; if so, it may be able to obtain a reference to
the new instance directly from the superceded agent, or query the
kernel for the reference.

\Scheme only reclaims objects (in this case instances of a submodel)
when all references to the object have vanished; this means that as
long as an agent maintains a reference to the entities it is
interested in, those entities will persist, and can be used to obtain
a reference to instances which have replaced it.

Agents representing animals or plants will typically already know
about their ``domain'' -- the landscape they inhabit.  Landscapes are
usually cast as bounded geographic regions associated with
environmental data such as water availability, topography or
contaminant levels. In contrast, an animal would typically \emph{not}
keep track of other animals.  The agents in the example model only
cache a limited amount of information in this regard.

The basic calls which return the locations associated with agents look
like
\begin{verbatim}
...
   (let* ((prey (kernel 'locate 
                  (apply *provides-*? (my 'food-list))))
          (cover  (look-for self (my 'prey-hides)) )
            ;; plants that can hide my prey
          (preylocations 
            (map (lambda (x) 
                   (query x self 'location 
                           (my 'search-radius))) 
                 (append prey cover)))
         )
         ...
)
...
\end{verbatim}
In this example, the first of the three clauses obtains a list of
candidate prey with a call to the kernel\footnote{In this case
  \symb{locate} returns all possible candidates irrespective of
  location, but insists that the targets are
  contemporaneous. \symb{locate*}, in contrast, does not enforce
  temporal consistency. Both are able to filter the results based on
  location, class or the role targets play in the simulation.}  The
next searches for potential hiding places with the \method{look-for*}
call. This call does not enforce synchrony\footnote{The alternative
  \method{look-for} does enforce synchrony.}, and the last actually
extracts targets' locations for subsequent use.

Once an agent is aware of another agent, it will typically interact
using either the \method{query} call or calls to the methods
appropriate to the target. \footnote{The MOP paradigm makes it possible to use
substantially the same code for many agents of many classes, but calls
through \method{query} may be easier to convert for parallelism later.}

\section{Changing representations}

The process of assessment and the management of the representations
of the systems being modelled is reasonably simple to describe:
periodically the monitoring agent will interrogate the subset of the
model for which it is reponsible:
\begin{itemize}
\item[] Each of the agents it manages will be queried for its status
and its own assessment of its state in the context of the other
elements it interacts with.  This may be fairly trivial for many
agents, particularly those which change slowly, or are not
significantly influenced by the activities of other agents.
\item[] The monitor synthesises a number of representative
state-trees, most importantly a state-tree which describes the current
configuration, and others dealing with significant conditions which
pertain to the simulation -- perhaps relating the geographic
distributions, population numbers or density, the abundance of
resources, or the presence of significant conditions such as fire.
{\Huge{HERE}}
\end{itemize}


\subsection{Maintaining state across representations}

When an entity changes its representation, its current representation
may create an encapsulation of its critical state variables in a
closure that is bound to a function which is able to access these
variables, and to maintain their values. The closure may ultimately pass these
values back to another agent which will take over the role played by
the model maintenance function.

A typical closure definition might look like
\Huge{NEEDS TO BE FIXED}
\begin{verbatim}
(define-update-closure 
    fish-population
    <FishPop> ;; used for identity checking
    '(age mass contaminant) ;; state variables 
    '(temp contlevel) ;; required external values
    (begin
       (set! mass (fishgrowth dt age mass (data-ref self 'temp))
       (set! age (+ age dt))
       (set! contaminant (fishcont dt mass contaminant contlevel))
       )))
\end{verbatim}
In this example the state-variables age, mass and the contaminant
level are all updated in the body; the update closure requires a
temperature, \sttvar{temp}, and a contaminant level, \sttvar{contaminant}.
The macro definition for \Syntax{define-update-closure} isn't
necessarily simple to follow.

\subsection{Using trees to assess model configurations}

The model periodically runs \mclass{monitor} agents which generate
``maps'' of the current configuration of the model as elements of
the metric-space of the trees of Chapter~\ref{treering}.  We use
this mapping to calculate the closest candidate configuration from a
set of configurations we believe (or know) to represent the system
well in a given part of its state space. Because the trees are
elements of a ring, we can, in some cases, interpolate between
configurations, arriving at possibly advantageous configurations which
have not been explicitly specified.

\subsection{Candidate configurations and state trees}\typeout{HERE: Unfinished business}

Developing a body of configurations which function well is likely to
be, at least initially, a process of intuitive selection and trial and
error. Initially, we begin with coarse knowledge regarding the utility
of particular representations under given conditions. \mloggers can
collect and record information about the agents within the simulation,
and their performance. This data can be used as the basis for the
selection and testing of new configurations, and we can classify them
into varying degrees of ``good'' and ``bad'' configurations.

This process depends on being able to discriminate between
configurations, and we do this by mapping configurations into
state-trees which are constructed in a way which encodes coarse
spatial information (the relationship between \mpatches, for example),
the numbers of entities represented by different classes, and their
own performance measures.

%\subsection{Mapping from interpolated configurations to actual
%  configurations}\typeout{HERE: Unfinished business}

%% \textbf{CLARIFY THIS}
%% Where closures are \emph{supposed} to be bound to communicate with
%% only the agent maintaining them, other entities may have other, more
%% direct, channels. Chief amongst these are the connections between
%% subsidiary agents and their containing supervisor.  These agents can
%% use class methods to query each other directly. The constraint imposed
%% on maintenance closures exists primarily because they really only
%% exist \emph{in potentia}, and the data they request may actually be
%% synthesised by the agent responsible for their maintenance, rather
%% than the agents which existed at the time the closure was created.

%% This is an uncomplicated system, more significant issues would arise
%% in a parallelised system.  

\section{Discussion}



\subsection{Conveniences}
A number of features of the model have proved to be extremely useful
during the construction of the model, and in playing with
it. Particularly noteworthy items are the parameterisation mechanism,
output generation, the instrumentation to collect the elapsed time
within routines, and the code for debugging output.

Parameter files are small snippets of \Scheme code which adhere to a
particular form. In these files, data are associated with symbols
which get used to determine what state variables are initialised to.
The mechanism which support delayed evaluation of the value to
initialise a state variable allows the modeller to perturb values
(such as mass) in the instantiation of the agents.  Similarly, we are
able to move functions that are traditionally coded as \emph{part of
  the model} into the parameter space -- we could easily write one
basic Galapagos finch model and parameterise the many different
behaviours and food preferences.

\vspace{3mm}
All of the output in the model arises from one of two basic
mechanisms: debugging/warning messages emitted by the ``kdebug''
routines which were briefly discussed in Section~\ref{kdebug} or
output generated by a \mlogger.  There have been a number of benefits
from not incorporating the output within the models.  The most
significant benefit is that it makes the task of producing ``exotic''
output, such as postscript or animations, much more straightforward,
and the code to produce the output is much less likely to interfere
with the model itself, since the output routines are quite distinct
from the models they may poll.  All an output routine needs to know is
how to obtain data from an agent, and how to generate appropriate
output. In this way, the \method{log-data} routines are able to be
applied to a number of quite different models, and their output is
consistent irrespective of the underlying model generating the data.

The debugging messages provided by the \method{kdebug} call and its
kin can be constructed to emit messages according to arbitrary flags
which can consist of symbols, can contain wildcards, class-names, or
parameter taxa.  The same mechanism can conditionally execute more
complex debugging or validation code, and virtually vanishes when the
no messages are required.  This has proved very useful both in
debugging code, and in the parameterisation of models.

\vspace{3mm}
Model execution is often dominated by a few parts of the code which
prove to be computationally intensive for one reason or another.  The
\filename{framework} file provides the \Syntax{define\%},
\Syntax{model-method\%} and \Syntax{model-body\%}
alternatives to \Syntax{define}, \Syntax{model-method} and
\Syntax{model-body}. These alternative add instrumentation to the code
which allows the modeller to determine how much time was actually
spent in the functions, methods or model-bodies, with a call
to \texttt{(timing-report)}'.  This make the task of finding the
bottlenecks in the code much simpler.



\section{Further work: cross-representation predation and interaction\label{predation5}}

The work oulined in this section has not been implemented in the
example model, but is included since the disparity between
individual-based predation and population-level predation may be a
frequent cause for representation changes, and it provides a good
example of how coupling and analytic component may be readily
accomplished in a discrete model. Models at the ``individual-based''
end of the spectrum can be very sensitive to fine changes in
parameterisations or gradients within the model.  In some contexts,
this is exactly what we wish for, but in others it can make tuning
the model difficult and there may be underlying structural factors
that have undue influence in the model's trajectory.\footnote{In an
  unpublished model I wrote in the very early 1990's, simulated
  fish lined up neatly on boundaries between the pixels of the digital
  elevation map that made up the seabed -- they maximised their
  access to prey by straddling the different domains.}
Also, processes like predation are computationally expensive when
large numbers of prey are present.  It is possible for us to
model predation between individual-based and population-based
representations, but to do so requires an approach quite unlike the
approaches of conventional individual-based models, where the pursuit,
capture and consumption may be explicitly simulated, or the approaches
of conventional age-class structured population models.

The time taken to simulate interactions between individuals in an
individual-based model is often a significant proportion of the total
run-time, particularly when there are explicit searches for partners
to interact with.  While individual-based interactions are typically
implemented or resolved by simulating (at least to some degree) the
real-world events which occur in the system being modelled, the
interactions between the components of populations are necessarily
less well resolved -- the focus becomes the aggregate effects of the
interaction, and these are often modelled either using integral
transforms (in the case of functional representations) or repeated
evaluations over the histograms representing the populations.

Population models suffer less from this particular
source of overhead since, at least in ecological modelling,
populations are often represented as arrays or function whose elements
or values are an aggregation of individuals with respect to particular
characteristics. In domains where there are a large number of prey,
this is likely to be both more computationally efficient and more
likely to reflect the dynamics in the system being modelled.  It is
easy to imagine conditions where we would like to model either
predator or prey as an individual, and the other as a population.
Relevant examples may be models of relic or marginal populations at
risk of local extinction, or populations where the life-history of
individuals has a bearing on the sustainability of the population as a
whole (such as female elephants).  

%We can map population histograms onto piecewise linear functions with
%the property that the partial integrals of the the linear functions
%correspond to the partial sums of histograms at each boundary
%in the histogram.  If populations are functions, then we can evaluate
%the consequences on the population.

Let us consider the folowing ``known'' attributes in a system
\begin{table}{ht}
  \begin{center}
  \caption{Symbols\label{symbls}}
    \begin{tabular}{ll}
      $m_a (l)$ & the member distribution with respect to size of each agent of interest\cr
      $G_{i,j} (l, w)$ & the gape filter for predator $i$ with respect to prey \(j\)\cr
      $T (a)$ & returns the taxon or type number of agent $a$\cr
      $M_a$ & the total number of members for agent $a$\cr
      $M^{\ast}_i (w)$ & the sum of all the distributions of agents with a type $i$\cr
      $\bar{M}_i = \int_0^{\infty} M^{\ast}_i (w) d w$ & the total number of beasties of type $i$\cr
    \end{tabular}
  \end{center}
\end{table}

\subsection{Calculating mortality}

Let
\[ I_{i,j} (l, w) = M^{\ast}_i (l) G_{i,j} (l, w) \]
and
\[ J_{i,j} (l, w) = M^{\ast}_j (w) G_{i,j} (l, w) . \]
$I_{i,j}$ is the raw distribution of pressure of predator $i$ onto prey $j$,
and $J_{i,j}$ is the raw distribution of the vulnerability of prey $j$ to the
predator $i$.

If the constants
\[ k_{i,j} = \int_0^{\infty} \int_0^{\infty} I_{i,j} (l, w) dl dw \]
and
\[ h_{i,j} = \int_0^{\infty} \int_0^{\infty} J_{i,j} (l, w) dl dw \]
are non-zero, they can be used to scale $I_{i,j}$ and $J_{i,j}$ so that they
form kernel functions, and we get
\[ K_{i,j} (w) = \frac{1}{k_{i,j}} \int_0^{\infty} I_{i,j} (l, w) dl \]
which is the normalised predatory pressure with respect to size, and the
normalised vulnerability
\[ H_{i,j} (w) = \frac{1}{h_{i,j}} \int_0^{\infty} J_{i,j} (l, w) dl . \]
Values of zero in $k_{i,j}$ and $h_{i,j}$ indicate that no predation is
possible -- usually because $M^{\ast}$ has collapsed. In this case we take
either (or both) $K_{i,j} (w)$ and $H_{i,j} (w)$ to be zero.

We calculate
\[ v_{i,j} = \int_0^{\infty} K_{i,j} (w) H_{i,j} (w) dw \]
which has a value in the range $[0, 1]$. If $v_{i,j}$ is non-zero we can
construct the normalised interaction
\[ V_{i,j} (w) = \frac{1}{v_{i,j}} K_{i,j} (w) H_{i,j} (w) \]
which indicates the proportion by size of type $j$ subjected to predation from
type $i$ at the given length $w$. Again a zero value for $v_{i,j}$ indicates
that no interaction (``diner'' or ``dinner'') are possible.



The function
\[ e_{i,j} (w) = M^{\ast}_j (w) V_{i,j} (w)  \]
can be used to give us the the number
\[ E_{i,j} = \int_0^{\infty} M^{\ast}_j (w) V_{i,j} (w) d w \]
which is the exposure of prey population $j$ to the predators in
population $i$. The converse,
\[ C_{i,j} = \int_0^{\infty} M_i^{\ast} (w) V_{i,j} (w) d w, \]
is the potential for predation of the predator type $i$ on an ``average'' prey
of type $j$.

We can then use a predation relationship of some sort to get the raw
number of ``kills'' based on the exposure averaged over the potential
volume (or area) of contact per unit of time, which we call $\Omega_{i,j}$, where $\Omega_{i,j} = \bar{M}_i \mathfrak{F} (E_{i,j} / A_j,
p_{i,j}) \Delta t$ where $\mathfrak{F}$ is the predation relationship,
and $p_{i,j}$ is the parameterisation for the species, and $\Delta t$
is the time step, and $A_j$ is the area/volume we divide by to get a
density.

We can sum over a predator type
\[ \Omega_j^{\ast} = \sum_i \Omega_{i,j} \]
to give us the total possible consumption of prey type $j$.

Alternatively, we can calculate a consumption-by-size distribution and define
the function $\omega_{i,j} (w)$, the raw number of kills for a length $w$ on a
consumption-by-size basis, by
\[ \omega_{i,j} (w) = \bar{M}_i \mathfrak{F} (e_{i,j} (w) / A_j, p_{i,j})
   \Delta t. \]
Thus the impact on the prey population (at least those of length $w$) is
\[ \omega_j^{\ast} (w) = \sum_i \omega_{i,j} (w) \]
and for the whole population it is
\[ \int_0^{\infty} \omega^{\ast}_j (w) d w \]
(or something like that).

Alternatively, we can express things more in the way that it is
calculated in the Atlantis model (\cite{Fulton2011pcomm}) with
\[ Z_i (w) = \frac{g_i M^{\ast}_i (w)}{g_i / c_i + \sum_j a_{i,j} e_{i,j} (w)}
\]
where $Z_i (w)$ reflects the aggregate clearance rate of a predator of type
$i$ if we take $c_i$ to be the ``clearance rate'' which incorporates the
volume it sweeps and a proportion of prey captured and we take $g_i$ to be the
predator's growth rate.

So $\int_0^{\infty} Z_i (w) e_{i,j} (w) \Delta t d w$ is the amount of prey of
type $j$ consumed by the predators of type $i$ over the interval $\Delta t$.



It should be pointed out that the number of types is fairly small compared to
the number of agents, and this shouldn't be too onerous a calculation (at
least compared to playing it all out individually).

\section{Apportioning mortality}

Mortality can be calculated either by apportioning it to each agent according
to the proportion of the global population it represents (and within it,
apportioning the mortality to ages in an analogous fashion), or we can
apportion mortality to each age in each agent according to how much of the
population it represents.

For the agent-by-agent update we have
\[ \delta m_a (w) = \Omega^{\ast}_{T (a)}  \frac{m_a (w)}{M^{\ast}_{T (a)}
   (w)} \]
and for the age-by-age update we have
\[ \delta m_a (w) = \omega^{\ast}_{T (a)} (w)  \frac{m_a (w)}{M^{\ast}_{T (a)}
   (w)} . \]
The new distribution
\[ n_a (w) = m_a (w) - \delta m_a (w) . \]
In these steps, places where there are no members of size $w$ should be dealt
with carefully in the division, and at all points $M^{\ast}_{T (a)} (w)
\geqslant m_a (w)$.






%% \subsection{Comparison of states}
%% The first step in being able to quantitatively compare configurations
%% of models is to construct a means for encoding the state in a way that
%% supports some sort of qualitative or quantitative comparison.  The
%% trees described briefly in Chapter \ref{adaptiveselection} and more
%% comprehensively in Chapter \ref{treering} are elements of a metric
%% space. Since the set of indeterminate variables in the labels of these
%% trees is quite arbitrary, we can identify cells and groups of cells
%% explicitly with indeterminates, and we can also extend this to
%% denoting representations which are in use or may be used.

%% \subsection{Partial ordering of trees and nodes}\label{partial-order}
%% We can construct a partial order on nodes relatively simply.  In the
%% first instance we normalise the polynomial labels if necessary (by
%% collecting like terms) and sort nodes using their labels, where the polynomial labels are
%% ordered canonically with indeterminate factors in each term sorted by
%% lexicographic order, and the terms then sorted by degree.\footnote{Of course
%% other sorting strategies are possible.} If two nodes, possess the same
%% label, they are then ordered by their weights.  Should the weights
%% \emph{also} match, they are sorted on the relative order of their set
%% of children, with empty sets taking precedence over sets with
%% children. We may have to exercise some care in ensuring that the
%% lexical ordering doesn't unduly influence our ranking of different
%% configurations. 

%% The sorting order makes the construction of regular, readable output
%% straightforward, since it largely conforms to one of the common
%% patterns used in mathematics and computer science. 
