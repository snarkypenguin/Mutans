% chap5.tex (Chapter 5 of the thesis)


\chapter[AN EXPLICIT IMPLEMENTATION]{An explicit implementation}
\WeAreOn{\cfive}\label{explicitmodel}

Chapter~\ref{modelefficiency} argued that switching models are worth
considering because they may provide benefits in fidelity and
efficiency. The paper demonstrated significant advantages over
non-switching versions of the the system.  The hypothetical example
developed in Chapter~\ref{adaptiveselection} attempts to flesh out a
more fully formed representation of a small trophic network, and to
provide a platform for a discussion of how a general model might be
constructed. Here, we describe an explicit implementation of a system
based on Chapter~\ref{adaptiveselection}. The role of this
implementation is not primarily to provide a complete, efficient
implementation of a framework supporting adaptive hybrid models;n
rather, it is the starting point for further exploration and
development.  In this chapter, we describe the system, discuss the
reasons certain approaches have been taken, and address both how the
system could be used when implementing new models and avenues for
further research.

\section{Design}
In general, the design of this example framework is a conventional
agent-based model with arbitrary time steps. Its overall design was
influenced by my previous work in modelling the interactions of human
activity and marine ecosystems (\cite{lyne1994pmez5, gray06:1,
  gray2014}). This work takes as its starting point the scheduling
strategies which allow modelled entities to adjust their timesteps on
a step\-by\-step basis according to their circumstances, a floating
point representation of time and space, and the notion that behaviour
is most readily represented in a modal fashion --- animals, for
example, would \emph{sleep}\/, \emph{flee}\/, and \emph{forage}\/ as
quite distinct activities. Implicit in the modal approach to behaviour
is a mechanism for selecting an appropriate type of behaviour for the
entity's context. In some sense, changing a representation of a
submodel is qualitatively similar to the selection of an appropriate
mode of behaviour; the major differences lying in the level of
support required from the underlying model framework.

The mechanisms for changes in representation and the maintenance of
state for superceded representations evolved from the basic strategy
developed in Chapter~\ref{modelefficiency}.  Since the only
interactions in Chapter~\ref{modelefficiency} were between the
simulated organisms and the environmental plume, there was no need to
include mechanisms for agents to interact in a more general
sense. Much of the code in the example model is devoted to providing
the basic mechanisms with which models of various entities,
environmental domains may interact, and to ensure that the operation
of the systems is temporally coherent, consistent and readily
extended. 

The ``processing loop'' which governs the simulation of each modelled
entity is directly analogous to the simpler UNIX kernels of decades
past---there is only one entity actually being simulated at any point
in time, but they are interleaved in such a way as to minimise
inappropriate bias and avoid too much temporal disparity.

The choice of implementation language was more difficult. I chose
\textsf{Scheme} over the more obvious candidate, \textsf{C++}.
While I had extensive experience in modelling in \textsf{C++}, and a significant body of
code from which I might draw, the considerations which prompted me
to use \textsf{Scheme} in the paper of Chapter~\ref{modelefficiency} still
pertained. While \textsf{Scheme} must be considered a minor language in
industry\footnote{It was ranked $43^{\text{rd}}$ in the IEEE Spectrum 2016 list
of programming languages}, in most of the more common languages it
is difficult to cleanly and generically implement an analogue to the
closures used in Chapter~\ref{modelefficiency} to maintain attributes
possessed by superceded representations.  \textsf{Scheme} is active developed,
there are a number of very good interpreters and compilers for it;
many of these implementations are also amenable to integration with
other languages, either through the linking binary objects or by
interacting through a virtual machine (such as a JVM).
\textsf{Scheme} was, at its inception, designed to be a tool for exploration
\cite{sussman1998first} and it remains an excellent tool for this
purpose.

The class structure in the example is based on the \textsf{Scheme}
implementation of \textsf{tiny-CLOS} (\textsf{SCLOS}) that was written
by Gregor Kiczales \cite{kiczales1993xerox} while he was working at
Xeros PARC in 1992 and 1993.  Kiczales has been an instrumental
researcher and proponent for the use of metaobject protocols (MOPs) as
a tool for making computer programs clearer, more efficient, and more
robust.  The basic tenet is that generic methods or functions are used
to manipulate objects, and that these generic objects inspect the
nature of the data being passed to them and pass the processing to a
specialised function which deals with the task most appropriately.
While this is not an exact analogue to the problems we seek to address
in this work, there is enough commonality that using \textsf{SCLOS}
and its implicit MOP seemed a natural fit.  \textsf{SCLOS} has been
the basis on which many of the significant object systems for
\textsf{Scheme} have been built.

I have used \textsf{Gambit-C} (\textsf{Gambit}), a well regarded
implementation of \textsf{Scheme} which runs on all of the major
platforms (Linux, Unix, OSX, and Android, for
example). \textsf{Gambit} incorporates both a fully developed
\textsf{Scheme} interpreter and a compiler, and both can readily link
programs with external code and libraries which may exist either as
compiled or as interpreted \textsf{Scheme} code. The choice to use
\textsf{Gambit}, rather than another version of \textsf{Scheme}, was
heavily influenced by several points: it was the implementation used
for the model in Chapter~\ref{modelefficiency}; its compiled code can
be significantly faster than the interpreted code; it supports very
\textsf{C}-like infix notation; and adding hand-crafted \textsf{C} or
\textsf{C++} code for particularly intensive routines is not
difficult.  Porting the model to other versions of \textsf{Scheme}
should be relatively simple; the only potentially awkward issue in
using a different \textsf{Scheme} implementation would be the use of
\texttt{define-macro} which is used to provide syntactic structures
which are able to detect and respond to inconsistencies and makes
using objects simpler and less error prone\footnote{At least for
  \emph{some} types of error!}.  In principle, \texttt{define-syntax}
and other tools for syntactic extension or modification might be
substituted for the macros defined in the \texttt{framework} file.

\subsection{Comparison of states}
The first step in being able to quantitatively compare configurations
of models is to construct a means for encoding the state in a way that
supports some sort of qualitative or quantitative comparison.  The
trees described briefly in Chapter \ref{adaptiveselection} and more
comprehensively in Chapter \ref{treering} are elements of a metric
space. Since the set of indeterminate variables in the labels of these
trees is quite arbitrary, we can identify cells and groups of cells
explicitly with indeterminates, and we can also extend this to
denoting representations which are in use or may be used.

\subsection{Partial ordering of trees and nodes}\label{partial-order}
We can construct a partial order on nodes relatively simply.  In the
first instance we normalise the polynomial labels if necessary (by
collecting like terms) and sort nodes using their labels, where the polynomial labels are
ordered canonically with indeterminate factors in each term sorted by
lexicographic order, and the terms then sorted by degree.\footnote{Of course
other sorting strategies are possible.} If two nodes, possess the same
label, they are then ordered by their weights.  Should the weights
\emph{also} match, they are sorted on the relative order of their set
of children, with empty sets taking precedence over sets with
children. We may have to exercise some care in ensuring that the
lexical ordering doesn't unduly influence our ranking of different
configurations. 

The sorting order makes the construction of regular, readable output
straightforward, since it largely conforms to one of the common
patterns used in mathematics and computer science. 

\section{Model implementation}
The model is largely similar to the example used in Chapter
\ref{adaptiveselection}. In the hypothetical example, the domain
consists of nine cells containing tree-like plants. These trees are a
critical food source for a population of herbivores, and the trees
rely on the young herbivores to eat the ripe fruit in order to make
their seeds viable (perhaps the seeds have a particularly tough
seedcoat). A carnivore which preys solely on juvenile herbivores is
introduced into a stable system.  In Chapter~\ref{adaptiveselection},
the introduction of the carnivore and a subsequent rangeland fire
initiated a sequence of events which, in turn, engendered
changes in the configuration of the model as a whole.  The
implementation described here is not identical to the system described
in the paper---some inadequacies of the model presented in the paper
have been addressed (such as the inability of the plants to recolonise
a cell after they have become extinct in it).

The primary purpose of the framework is to aid construction and
exploration of models, rather than as a pedagogic aid in teaching
object-oriented programming. As a result, some of the basic machinery
that one would typicall need to use in \textsf{SCLOS} is available via
macros which provide
\begin{itemize}
   \item syntactic sugar to make the code more readable, and to simplify
     definitions of model components \\
   \item targetted checks in the definition of methods which catch
     errors which are particularly opaque and difficult to isolate,\\
   \item a means of constructing ``maintenance closures'' for
     representations of submodels which are about to cede their place
     to new representations
\end{itemize}
and
\begin{itemize}[resume]
\item support for straighforward chaining back through sets of
  a submodel's parent classes.
\end{itemize}

Representations of submodels are constructed as sets of state
variables, a set of appropriate \texttt{model-methods}, and a
\texttt{model-body} which embodies their control loops. 

\subsection{Initialisation}

The first stage in starting a model run is the creation and
initialisation of the components of the model. Start a model run would
usually involve running a short \texttt{Scheme} program which uses the
model infrastructure to create and configure agents which comprise the
model ensemble.  These agents are added to a list called the runqueue,
which corresponds to the execution queue in a multitasking operating
system. The agents are usually created (typically by calling \texttt{(create
  \textit{<classname> \textit{taxon}}\/\ldots)} where the ellipsis indicates
a set of appropriate state variable assignments.  Sometimes additional
machinery is required to add agents to encompassing entities (such as
including trees in the landscape).

The principal tasks of \texttt{create} are to allocate storage, and to
call the functions which initialise the state variables of an agent.
\texttt{Create} initialises its state variables using the data from
the files in a nominated parameter directory. The state variable
initialisations specified in class--based files are applied first from
the parent class farthest away in inheritance, through to file
associated with class of the agent being created. It is quite likely
that a given state variable may be reset several times in the process
as the model representation is refined. Finally, the state variable
settings from a file associated with a \emph{particular}\/ set of
instantiations---the agent's taxon---are applied.  A model might deal
with several populations of distinct species (taxa) of tuna.
These taxa may have different prey preferences, temperature preferences, and
metabolic parameters.  Each distinct species would load its
initialisation parameters from a taxon file associated with the
species.

In order for the initialisations indicated in these files to be
applied in the correct order, class--associated files must have the
same name as the class they relate to (such as ``\texttt{<tuna>}''),
and taxon--associated files will have the same name (including case)
as the taxon used (such as ``\texttt{T.albacares}''.\footnote{It
  doesn't matter if there are entries for things which are not state
  variables---any specification which isn't recognised is silently
  ignored.}

Parameter files contain \textsf{Scheme} code and they are loaded
immediately after all of the code associated with implementing
representations of submodels has been loaded, but before any model
initialisation occurs.  A typical parameter file might look
like look like the '\texttt{<example-plant>}' file below:
\begin{verbatim}
'Parameters
(define <example-plant>-parameters
  (list
    (list 'cell '<uninitialized>)
    (list 'location '(@ (random-location '(-2000 -2000) '(2000 2000))))
    (list 'peak-mass 12)                 ;; this will seed trees in the
                                         ;; 4km x 4km square around
                                         ;; the origin
    (list 'mass (@ (nrnd 4 1 0.01 12)))
    (list 'fruiting-mass (* 8 kg))
    (list 'fruiting-prob (/ 0.1 week))         ;; 10% per week
    (list 'fruiting-rate (* 7 (/ 1 kg))))      ;; seven fruit per kg of tree
    (list 'mort-mass '<uninitialized>)
    (list 'mort-prob (/ 0.1 year))
    (list 'seeds-per-fruit '(@ (+ 4 (urnd 8)))
    (list 'habitat '<uninitialized>)
    (list 'references '(J. Muppet Botany, S.Chef et. al. v2,1995))
    (list 'note "This is enough for things to run")
  )
)
(set! global-parameter-alist 
   (cons (cons <example-plant> <example-plant>-parameters) 
         global-parameter-alist))
\end{verbatim}
The \texttt{'Parameter} line which is the first entry in the file is
an indicator that this is a parameter file, and \textsf{Scheme}
programs (like the modelling framework) can readily use this to
determine that the file is not actually code intended to be executed.
The definition which follows is a list of lists where the first
element of each of the second level lists must be a symbol.  These
symbols should (but are not required to) correspond to the state
variables in the agent. Note that even though this file is associated
with '\texttt{<example-plant>}', it is not restricted to setting state
variable defined in \texttt{<example-plant>}---it is \emph{reasonable
  and common} to set variables associated with parent classes, such as
its \texttt{location}, above, which ultimately comes from the \texttt{<thing>}
class.

The last three lines of the file add the definitions to the list of
all the state variable defaults the system knows about---if they are
missing or compromised, the definitions may not be accessible.

Most of the expressions in parameter files are evaluated while the
file is being loaded, but there are exceptions.  Any item with a
single quote will not be evaluated during the loading process, so the
list associated with ``\texttt{'references}'' is preserved, similarly
the list associated with the number of seeds per fruit, ``\texttt{(\at
  (+ 4 (urnd 8)))}'', is read verbatim, but will be evaluated each
time an agent is instantiated, so that each tree will produce seeds at
its own rate from between four and twelve seeds per fruit.  The
framework uses the ``\texttt{\at}

\subsection{Configuration seeds}

\subsubsection{Known--good and known--bad status-configurations}

\subsubsection{Operational configurations}


\subsection{Assembling a model}
Models in the framework are assembled by constructing a small scheme
program which creates the agents and introduces them into the
runqueue. The set of agents would typically include
\texttt{<monitors>} which will periodically assess the state of the
model and its components with the charge of changing the configuration
if things drift too far from its preferred region of the state-space
of the model.  The model also loads the sets of known--good
and known-bad corpora which serve to inform the assessments of the
monitoring agents.





\section{Execution}

The framework provides for the ability to make an agent
``subsidiary'' to another agent in one of two ways: the subsidiary
agent may remain a member of the global runqueue---only notionally
``contained'' by its supervisor, or it may be wholly subsidiary and
control only passes to it during the time step executed by its
supervisor. This second mechanism is targetted at situations where a
modelled entity may be comprised of several submodels, such as an
animal with different possible representations for its metabolism. 

In the case of the example of Chapter~\ref{adaptiveselection}, we
explicitly associate the agents representing trees with particular
cells---trees populate the cells, notionally contained, but
introduced in their own right into the runqueue.  Background
quantities of fruit and seeds are introduced into the model as numeric
values associated with the cells, and a number of herbivores, adult
and juvenile, are also generated and placed at locations within the
domain. An initial population of carnivores is also generated, but
they are given an initial time substantially after the start of the
run, so they will not have control passed to them till the rest of the
agents have caught up.

Additional agents within the system, the monitors and loggers, are
also instantiated and added to the queue. The monitors will
periodically assess the configurations of agents, ensembles of agents
and the model as a whole and make changes in the configuration, if
required.  The loggers will periodically poll the other agents and
produce some sort of output or record of data derived from the state
of the model.

After the initial cohort of agents have been instantiated and
introduced into the runqueue (or in the subsidiary-runqueues of other
agents), control is passed to the kernel which then begins to run each
agent in turn for an appropriate time step.

\subsubsection{Control flow}

The most important mechanism controlling the flow of control in the
system is the execution loop which plays the role of the kernel. A
sorted list of agents is maintained by the system; the head of this
list is removed from the list and execution is passed to
\texttt{model-body} of the agent with an indicated maximum amount of
time over which it might run.  Agents need not run their whole time
step---events may occur which cause them to truncate their turn,
returning control to the kernel with an indication of the amount of
time they actually used.  When the kernel receives control from an
agent, it examines the data passed to it by the agent and act
accordingly: dead agents may be silently dropped from the queue, for
example, and other agents may be reinserted in an appropriate place in
the queue for the their next time step.

Control also passes from agents to the kernel when the agent makes
queries about or of other agents: a sabre-toothed tiger might want to
know whether there are any prey animals within a certain radius.  A
request goes to the kernel for a list of nearby prey (transferring
control back to the kernel).  The kernel will examine the list of
agents meeting the requirements (spatial location, temporal
contiguity) and pass the list---and control---back to the tiger.
These control issues are conventional, both in terms of operating
system dynamics, and in agent-based modelling.

The maintenance of state data from a superceded representation is
rather different. When an agent has entries of maintenance data, it
processes each of them in turn by asking it for the data it needs,
obtaining the data and passing control and the data to the closure
When the closure has finished its update, control returns to the
agent, and it either continues in its time step or passes control back
to the kernel.

\subsubsection{Communication}

Where closures are \emph{supposed} to be bound to communicate with
only the agent maintaining them, other entities may have other,more
direct, channels. Chief amongst these are the connections between
subsidiary agents and their containing supervisor.  These agents can
use class methods to query each other directly. The constraint imposed
on maintenance closures exists primarily because they really only
exist \emph{in potentia}, and the data they request may actually be
synthesised by the agent responsible for their maintenance, rather
than the agents which existed at the time the closure was created.

This is an uncomplicated system, more significant issues would arise
in a parallelised system.  


\section{Classes and structure}

The example model follows a traditional agent-based approach using a
class hierarchy that reflects the entities we wish to model. Most of
the agent classes are organised more by their niche than by their
structural form---we may have several ``animal-like classes'' which
may range from individual-based representations to purely
equation-based representations. This is motivated by the observation
that the kinds of interactions an animal may engage in are
substantially different from the interactions associated with a plant,
for example.  In some cases an equation-based representation of an
animal may need to participate in what seems to be an
individual-to-individual interaction or visa-versa.  These situations
need to be examined in more detail and will be addressed in Section
\ref{interactions}


The framework is structured like an older multitasking,
multiprocessing operating system which supports an arbitrary number of
interleaved processes which may each have a different (varying)
priority and which may run for steps of an aribitrary length. There is
only one processing queue in the framework, so we can be certain that
only one agent will be executing at any given moment.  In principle,
the relatively small kernel could be replaced by a multithreaded
kernel without requiring major changes to the rest of the system. True
concurrent processing would require additional facilities for
inter-agent communication and ways to ensure that sets of agents could
become synchronous when necessary. Similarly, a number of processes
(such as numeric integration) might be readily parallelised.  In both
these instances, it was felt that these changes would come at a cost
in terms of the clarity of the code, and an increase in the difficulty
of debugging and exploration.

All of the agents and objects in the model are derived
from the \texttt{<primitive-object>} class in a slightly modified
\textsf{SCLOS}\footnote{This class is actually the \texttt{<object>} class in
  the canonical \textsf{SCLOS} (version 1.7), but I really wanted
  \texttt{<object>} to provide an entity which was like an agent, but
  without the ability to run as an independent dynamic element in the
  system---hence the name change}, this provides a clean distinction
between the classes which are associated only with the \textsf{SCLOS}
architecture and the classes associated with the model.  The basic
classes used to build the entities of the model are the
\TTC{object} class and the slightly more complex \TTC{agent}
class. \TTC{agent} is a subclass of \TTC{object} and has the ability to
run as a process in the run-queue, and to interact directly with the
kernel and with other agents in the system.  \TTC{object}s lack that
connection to the kernel, and are only able to operate on their own
data, unless assistance is provided by some external agency.  An
example of the distinction between these classes is that a \TTC{patch}
is a spatially explicit \TTC{agent} element which is primarily used to
give context to ecoservices, and its geographic footprint, a polygon
or circle, is represented by an \TTC{object} which can answer queries
such as ``How far is $p$ from your perimeter?'' or ``How much area do
you contain?'' 

A brief summary describing the role of the classes and their taxonomic
relationships 

\begin{description}
\item[\TTC{object}]---This class sits at the top of the taxonomic tree
  for all the significant classes in the modelling framework.

  \begin{description}
  \item[\TTC{model-maintenance}]---These objects (not agents) contain
    functions with closures which may be used to maintain the data an
    agent needs to preserve across transitions.  An example of this is
    the contaminant vector preserved across representations in the model
    of Chapter~\ref{modelefficiency}.  The function will respond to a
    number of ``flags'', namely \texttt{'needs?}, \texttt{'state},
    \texttt{'dump} and \texttt{'update}.

  \item[\TTC{polygon}, \TTC{circle}]---These classes are used to
    delineate regions within the simulation.  

  \item[\TTC{simple-metabolism}, \TTC{metabolism}]---These classes
    provide the metabolic component of the animal models.
    
    
  \item[\TTC{agent}]---Most submodels or components of submodels
    will be derived from the agent class.  \TTC{agent} provides the
    basic structure for interacting with the system and with other
    agents.

    \begin{description}
    \item[\TTC{introspection}]---This class provides the basic machinery
      used for logging agents.  Logging agents will periodically poll
      lists of agents and generate output files consisting of things
      like numeric data or maps.

      \begin{description}
      \item[\TTC{logfile}]---The logfile class is the basis for the
        time-series output. Logfiles would typically be used to
        extract data from a modelled data logger, for example.
        
        \begin{description}
        \item[\TTC{log-data}]---Instances of the log-data class
          generate datafiles such as might be loaded into matlab or
          octave.
        \end{description}
        
      \item[\TTC{snapshot}]---This class produces sequential files
        which contain snapshots of the state of components of the
        system.  A snapshot approach is somewhat analogous to an
        aerial survey of rangeland species.
        
        \begin{description}
        \item[\TTC{log-map}]---This class generates (sequential)
          postscript or PNG maps as the model runs
        \end{description}
      \end{description}
    \end{description}

  \item[\TTC{monitor}]---Monitors are the components of the
    model that assess the configuration of the system and
    triggers changes

  \item[blackboard]---Blackboards are used to indicate
    conditions which may influence other agents in the system.
    If an agent enters a part of its state space where it
    performs poorly, it may post a notice on a blackboard
    indicating the condition.

  \item[\TTC{tracked-agent}]---Agents which have their
    \emph{path} tracked through time.  The path in question may
    be geographic, but it could also be some other state
    value, such as salinity, voter-sentiment, or location

    \begin{description}
    \item[\TTC{thing}]---This class is the basis for agents with
      specific locations. Individual members of a group, such as
      whales in a pod, health centres, or seismographs.
      
      \begin{description}
      \item[\TTC{simple-plant}]---This class is geared to
        represent simple tree-like plants.

      \item[\TTC{animal}, \TTC{simple-animal}]---These classes
        provide the basic animal models.  The animal classes
        inherit from \TTC{metabolism} or \TTC{simple-metabolism},
        respectively. 
      \end{description}


    \item[\TTC{population}]---Implementations of populations
      could range from ``super-individuals'' in the sense of
      ~\citet{scheffer1995super} through age structured matrix
      models and systems of differential equations. The major 
      identifying characteristic is that populations cover an
      area, rather than a distinct location, and the inherent time
      steps associated with populations are typically longer
      than those used for subclasses of \TTC{thing}.
    \end{description}

  \item[\TTC{ecoservice}]---Ecoservices correspond to resources
    which may be used or generated by other agents.  They might
    represent anything from local groundwater to numbers of visitors
    in a museum.  Though they are included with the
    ``landscape'' classes, ecoservices are not inherently tied to
    locations.

    \begin{description}
    \item[\TTC{population-system}]---Population-systems are a
      subclass of ecoservices, and is tailored to representing
      simple equation based population dynamics within a spatial
      domain (a dynamic-patch).
    \end{description}

  \item[\TTC{patch}, \TTC{dynamic-patch}]---These classes associate a
    geographic region (a circle or polygon) with a set of
    ecoservices or a dynamic system of equations.

  \item[\TTC{environment}]---environments provide the spatial context
    for a model.  Classes which actually reflect environmental
    features (biotic or abiotics, according to the inclination of
    the modeller) would be derived from the environment class.

    \begin{description}
    \item[\TTC{landscape}]---A landscape associates a terrain
      (either a function or some sort of digital elevation model)
      with an environment.
      
      \begin{description}
      \item[\TTC{habitat}]---Habitats are collections of patches
        which can be viewed as comprising the landscape.
        \begin{description}
        \item[\TTC{habitat*}]---This class adds a ``global patch''
          which encompasses the patches inherited from \TTC{habitat},
          and the global patch acts as an aggregated proxy for the
          more finely resolved elements.
        \end{description}
      \end{description}
    \end{description}
  \end{description}
\end{description}


\subsection{Methods, model bodies and closures}

Methods are functions which are specifically associated with instances
of a class.  \textsf{SCLOS} supports having a number of distinct methods
with the same name, but different arguments, as determined by their
type.  This means that \texttt{Seal} and \texttt{Shark} may both have an
\texttt{Eat} method---moreover, sharks may have two \texttt{Eat}
methods: one which recognises when the item being eaten is an
\texttt{Animal}, and one where it is merely an \texttt{Object}, which
would usually be inedible.  This multiple-dispatch means that no
explicit test is required in the model code (beyond writing an
``object'' version which spits out the offending item).  If there is
only an \texttt{Fish} verion of the \texttt{Eat} method for seals,
they can only eat fish.  This makes the extension of classes much less
vulnerable to mishandling interactions with instances of other
classes.  These class methods are the primary mechanism for
implementing behaviour in submodels in the system.

Model bodies are methods which are only called by either the kernel
or by the subsidiary execution lists embedded in agents. There is no
provision for an agent to directly call model bodies.

Closures have been discussed before as mechanisms for implementing
``update function'' which preserve the essential state variables for
particular representations.  A typical closure definition might look like 
\begin{verbatim}
(define-update-closure fish-population
                       <FishPop>
                       '(age mass contaminant) ;; state variables 
                       '(temp contlevel) ;; required external values
                       (begin
                         (set! mass (fishgrowth dt age mass temp))
                         (set! age (+ age dt))
                         (set! contaminant (fishcont dt mass contaminant contlevel))
                         ))
\end{verbatim}
In this example the state-variables age, mass and the contaminant
level are all updated in the body; the update closure requires a
temperature, \texttt{temp}, and a contaminant level, \texttt{contaminant}.
The macro definition for \texttt{define-update-closure} isn't
necessarily simple to follow, a simple example of a closure might look
like so:
\begin{verbatim}
(define counter 
   (let ((n 0))
      (lambda x 
         (cond
           ((null? x) (set! x (+ 1 x))
           ((eqv? x '(reset)) (set! x 0))
           (#t counter))))
           )
)
\end{verbatim}
The symbol \texttt{counter} is set to point to the function the
\texttt{lambda} defines, and the state variable \texttt{counter} is
``global'' to this function, rather like a static variable defined
outside a function in a C source file. 
             


Functions with closures are used, most importantly, to implement the
code run with the \TTC{model-maintenance} class. Here, an agent will
create a closure containing a function which is able to access the
variables within the closure, and maintain their values; it may
ultimately pass these values back to another agent which will take
over the role played by the model maintenance function.  Indeed,
\textsf{Scheme} was chosen specifically because these functions are a natural
representation for a fragment of a model which needs to be
maintained.\footnote{In principle, each class could be comprised of such
fragments from the outset, but it seemed that the purpose of the
work would be better served by a more traditional architecture.}

Initialisation methods are run when instances of classes are created
using the \texttt{make} function.  Two types of these functions occur
in the code---\texttt{initialize} with a \emph{z} and
\texttt{initialise} with an \emph{s}---and the distiction is that
the \emph{z} versions are associated with the \textsf{SCLOS} initialisation
chain, and the \emph{s} version is associated with the modelling
classes. When an instance is created the \TTC{object}
\texttt{initialize} is called, and this chains to the framework's
intitialisers.  When \TTC{object} derived class is instantiated, all of
its parent initialisers (with an \emph{s}) are called.  The
initialisers are
\begin{description}
  \item[\texttt{default-object-initialisation} and
    \texttt{default-agent-initialisation}] which just initialise the
    state-variables for the object or agent (see \TTC{polygon} in the
    file ``landscape-methods.scm''). There is no scope for additional
    processing using the default initialisation.
  \item[\texttt{object-initialisation-method} and
    \texttt{agent-initialisation-method}] which defines a method which
    will be executed when the object is created.
\end{description}
In fact, the only significant differences between the \texttt{object-}
and \texttt{agent-} versions are the error messages and the textual
difference in the model code, but the different labels reinforce the
different roles of the entities they refer to.
    
Model methods are rather like simple functions, except that they are
explicitly associated with classes---there are several versions of
the model method \texttt{dump}, and each is tied to particular a class
and its descendants.  This ability to have several functions with the same
name is made possible by the use of ``generic-methods'' which
maintain a list of actual functions and information about the
arguments they expect.  The generic function is able to 
recognise the types of the arguments it is passed and to forward the
call on to the appropriate actual implementation.  There are two
consequences to this approach which ought to be mentioned
\begin{description}
  \item[Only define one generic method for each set of similar
    functions] Redeclaring a generic-method ``(define hunt
    (generic-method))'' cuts any previously declared ``hunt'' methods
    (generic or otherwise!) out of the model's code path. To this end,
    all declarations of generic methods are either in the
    ``declarations.scm'' file, which contains the declarations for
    most of the model-methods or in the ``framework-declarations.scm''
    file which declares the generic-methods for methods required by
    the lower levels of the framework.  Related to this is the problem
    of defining a method before you have a generic-method declared---
    the macros in ``framework'' should catch this.
  \item[Don't try and construct methods that match the same argument
    lists] Generally, \textsf{SCLOS} is very good at getting the right
    method for the job, but there are situations where it can be
    confusing for it.  If you have a situation where there are several
    possible desired code paths for essentially similar arguments, it
    is clearer to have an explicit selection made within the body of a
    single method.
\end{description}
 
Each agent's \texttt{model-body} is run by the kernel at each of the
agent's time steps. While there is an
\texttt{object-initialisation-method} corresponding to an agent's
\texttt{agent-initialisation-method}, there is no \texttt{object-body}
since objects are not able to be run by the kernel.

\section{Interactions}\label{interactions}

The time taken to simulate interactions between individuals in an
individual-based model is often a significant proportion of the total
run-time, particularly when there are explicit searches for partners
to interact with.  Population models suffer less from this particular
source of overhead since, at least in ecological modelling,
populations are often represented as arrays or function whose elements
or values are an aggregation of individuals with respect to particular
characteristics.

Interactions between individuals are typically implemented or resolved
by simulating (at least to some degree) the events which occur in the
system being modelled. In contrast, interactions between the
components of populations is necessarily less well resolved---the
focus becomes the aggregate effects of the interaction, and these are
often modelled either using integral transforms (in the case of
functional representations) or repeated evaluations over the
histograms representing the populations.

We can map population histograms onto piecewise linear functions with
the property that the partial integrals of the the linear functions
correspond exactly to the partial sums of histograms at each boundary
in the histogram.  If populations are functions, then we can evaluate
the consequences on the population.












\section{Maintaining state across representations}

Unfinished business

\section{Using trees to assess model configurations}

The model periodically generates a ``map'' of the current
configuration as an element in the space of the tree-ring elements of
Chapter~\ref{treering}.  We use this mapping to calculate the
closest candidate configuration from a set of configurations we
believe (or know) to represent the system well in a given part of its
state space. Because the trees are elements of a ring, we can, in some cases,
interpolate between configurations, arriving at possibly advantageous
configurations which have not been explicitly specified.

\subsection{Candidate configurations}

\typeout{HERE: Unfinished business}

\subsection{Generating current-state trees}

\typeout{HERE: Unfinished business}

\subsection{Mapping from interpolated configurations to actual
  configurations}

\typeout{HERE: Unfinished business}




\section{Further work: cross-representation predation}

This discussion has not been implemented in the example model
discussed in this section, but is included since the disparity between
individual-based predation and population-level predation may be a
frequent cause for representation changes.  It is possible for us to
model predation between individual-based and population-based
representations, but to do so requires an approach quite unlike the
approaches of conventional individual-based models, where the pursuit,
capture and consumption may be explicitly simulated, or the approaches
of conventional age-class structured population models which can often
be thought of as

\subsection{An individual-based and analytic approach}

Let us consider the folowing ``known'' attributes in a system
\begin{table}{ht}
  \begin{center}
    \begin{tabular}{ll}
      $m_a (l)$ & the member distribution with respect to size of each agent of interest\cr
      $G_{i,j} (l, w)$ & the gape filter for predator $i$ with respect to prey \(j\)\cr
      $T (a)$ & returns the taxon or type number of agent $a$\cr
      $M_a$ & the total number of members for agent $a$\cr
      $M^{\ast}_i (w)$ & the sum of all the distributions of agents with a type $i$\cr
      $\bar{M}_i = \int_0^{\infty} M^{\ast}_i (w) d w$ & the total number of beasties of type $i$\cr
    \end{tabular}
  \end{center}
\end{table}

\subsection{Calculating mortality}

Let
\[ I_{i,j} (l, w) = M^{\ast}_i (l) G_{i,j} (l, w) \]
and
\[ J_{i,j} (l, w) = M^{\ast}_j (w) G_{i,j} (l, w) . \]
$I_{i,j}$ is the raw distribution of pressure of predator $i$ onto prey $j$,
and $J_{i,j}$ is the raw distribution of the vulnerability of prey $j$ to the
predator $i$.

If the constants
\[ k_{i,j} = \int_0^{\infty} \int_0^{\infty} I_{i,j} (l, w) dl dw \]
and
\[ h_{i,j} = \int_0^{\infty} \int_0^{\infty} J_{i,j} (l, w) dl dw \]
are non-zero, they can be used to scale $I_{i,j}$ and $J_{i,j}$ so that they
form kernel functions, and we get
\[ K_{i,j} (w) = \frac{1}{k_{i,j}} \int_0^{\infty} I_{i,j} (l, w) dl \]
which is the normalised predatory pressure with respect to size, and the
normalised vulnerability
\[ H_{i,j} (w) = \frac{1}{h_{i,j}} \int_0^{\infty} J_{i,j} (l, w) dl . \]
Values of zero in $k_{i,j}$ and $h_{i,j}$ indicate that no predation is
possible---usually because $M^{\ast}$ has collapsed. \ In this case we take
either (or both) $K_{i,j} (w)$ and $H_{i,j} (w)$ to be zero.

We calculate
\[ v_{i,j} = \int_0^{\infty} K_{i,j} (w) H_{i,j} (w) dw \]
which has a value in the range $[0, 1]$. \ If $v_{i,j}$ is non-zero we can
construct the normalised interaction
\[ V_{i,j} (w) = \frac{1}{v_{i,j}} K_{i,j} (w) H_{i,j} (w) \]
which indicates the proportion by size of type $j$ subjected to predation from
type $i$ at the given length $w$. \ Again a zero value for $v_{i,j}$ indicates
that no interaction (``diner'' or ``dinner'') are possible.

\

The function
\[ e_{i,j} (w) = M^{\ast}_j (w) V_{i,j} (w)  \]
can be used to give us the the number
\[ E_{i,j} = \int_0^{\infty} M^{\ast}_j (w) V_{i,j} (w) d w \]
which is the exposure of prey population $j$ to the predators in
population $i$. \ The converse,
\[ C_{i,j} = \int_0^{\infty} M_i^{\ast} (w) V_{i,j} (w) d w, \]
is the potential for predation of the predator type $i$ on an ``average'' prey
of type $j$.

We can then use a predation relationship of some sort to get the raw
number of ``kills'' based on the exposure averaged over the potential
volume (or area) of contact per unit of time, which we call $\Omega_{i,j}$, where $\Omega_{i,j} = \bar{M}_i \mathfrak{F} (E_{i,j} / A_j,
p_{i,j}) \Delta t$ where $\mathfrak{F}$ is the predation relationship,
and $p_{i,j}$ is the parameterisation for the species, and $\Delta t$
is the timestep, and $A_j$ is the area/volume we divide by to get a
density. \

We can sum over a predator type
\[ \Omega_j^{\ast} = \sum_i \Omega_{i,j} \]
to give us the total possible consumption of prey type $j$. \

Alternatively, we can calculate a consumption-by-size distribution and define
the function $\omega_{i,j} (w)$, the raw number of kills for a length $w$ on a
consumption-by-size basis, by
\[ \omega_{i,j} (w) = \bar{M}_i \mathfrak{F} (e_{i,j} (w) / A_j, p_{i,j})
   \Delta t. \]
Thus the impact on the prey population (at least those of length $w$) is
\[ \omega_j^{\ast} (w) = \sum_i \omega_{i,j} (w) \]
and for the whole population it is
\[ \int_0^{\infty} \omega^{\ast}_j (w) d w \]
(or something like that).

Alternatively, we can express things more in the way that it is
calculated in the Atlantis model (\cite{Fulton2011pcomm}) with
\[ Z_i (w) = \frac{g_i M^{\ast}_i (w)}{g_i / c_i + \sum_j a_{i,j} e_{i,j} (w)}
\]
where $Z_i (w)$ reflects the aggregate clearance rate of a predator of type
$i$ if we take $c_i$ to be the ``clearance rate'' which incorporates the
volume it sweeps and a proportion of prey captured and we take $g_i$ to be the
predator's growth rate.

So $\int_0^{\infty} Z_i (w) e_{i,j} (w) \Delta t d w$ is the amount of prey of
type $j$ consumed by the predators of type $i$ over the interval $\Delta t$.

\

It should be pointed out that the number of types is fairly small compared to
the number of agents, and this shouldn't be too onerous a calculation (at
least compared to playing it all out individually).

\section{Apportioning mortality}

Mortality can be calculated either by apportioning it to each agent according
to the proportion of the global population it represents (and within it,
apportioning the mortality to ages in an analogous fashion), or we can
apportion mortality to each age in each agent according to how much of the
population it represents.

For the agent-by-agent update we have
\[ \delta m_a (w) = \Omega^{\ast}_{T (a)}  \frac{m_a (w)}{M^{\ast}_{T (a)}
   (w)} \]
and for the age-by-age update we have
\[ \delta m_a (w) = \omega^{\ast}_{T (a)} (w)  \frac{m_a (w)}{M^{\ast}_{T (a)}
   (w)} . \]
The new distribution
\[ n_a (w) = m_a (w) - \delta m_a (w) . \]
In these steps, places where there are no members of size $w$ should be dealt
with carefully in the division, and at all points $M^{\ast}_{T (a)} (w)
\geqslant m_a (w)$.


